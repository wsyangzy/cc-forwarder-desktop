package tracking

import (
	"context"
	"database/sql"
	"fmt"
	"log/slog"
	"strings"
	"sync"
	"time"
)

// ArchiveEvent å½’æ¡£äº‹ä»¶ï¼ˆè¯·æ±‚å®Œæˆæ—¶å‘é€ï¼‰
type ArchiveEvent struct {
	Request   *ActiveRequest
	Timestamp time.Time
}

// ArchiveManagerConfig å½’æ¡£ç®¡ç†å™¨é…ç½®
type ArchiveManagerConfig struct {
	ChannelSize   int           // å½’æ¡£é€šé“å¤§å°ï¼ˆé»˜è®¤1000ï¼‰
	BatchSize     int           // æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤100ï¼‰
	FlushInterval time.Duration // åˆ·æ–°é—´éš”ï¼ˆé»˜è®¤5ç§’ï¼‰
	MaxRetry      int           // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰
}

// DefaultArchiveManagerConfig è¿”å›é»˜è®¤é…ç½®
func DefaultArchiveManagerConfig() ArchiveManagerConfig {
	return ArchiveManagerConfig{
		ChannelSize:   1000,
		BatchSize:     100,
		FlushInterval: 5 * time.Second,
		MaxRetry:      3,
	}
}

// ArchiveManager ç®¡ç†å½’æ¡£å†™å…¥
type ArchiveManager struct {
	archiveChan chan *ArchiveEvent
	adapter     DatabaseAdapter
	config      ArchiveManagerConfig
	pricing     map[string]ModelPricing       // æ¨¡å‹å®šä»·ç¼“å­˜
	endpointMu  map[string]EndpointMultiplier // ç«¯ç‚¹å€ç‡ç¼“å­˜
	location    *time.Location

	// çƒ­æ± å¼•ç”¨ï¼ˆç”¨äºå½’æ¡£æˆåŠŸåæ¸…ç†ï¼‰
	hotPool *HotPool

	// ç»Ÿè®¡ä¿¡æ¯
	stats ArchiveStats

	// ç”Ÿå‘½å‘¨æœŸæ§åˆ¶
	ctx    context.Context
	cancel context.CancelFunc
	wg     sync.WaitGroup
}

// ArchiveStats å½’æ¡£ç»Ÿè®¡ä¿¡æ¯
type ArchiveStats struct {
	mu               sync.RWMutex
	TotalReceived    int64         `json:"total_received"`     // ç´¯è®¡æ¥æ”¶äº‹ä»¶æ•°
	TotalArchived    int64         `json:"total_archived"`     // ç´¯è®¡å½’æ¡£æˆåŠŸæ•°
	TotalFailed      int64         `json:"total_failed"`       // ç´¯è®¡å½’æ¡£å¤±è´¥æ•°
	TotalDropped     int64         `json:"total_dropped"`      // ç´¯è®¡ä¸¢å¼ƒæ•°ï¼ˆé€šé“æ»¡ï¼‰
	TotalBatches     int64         `json:"total_batches"`      // ç´¯è®¡æ‰¹æ¬¡æ•°
	AvgBatchSize     float64       `json:"avg_batch_size"`     // å¹³å‡æ‰¹æ¬¡å¤§å°
	LastFlushTime    time.Time     `json:"last_flush_time"`    // æœ€ååˆ·æ–°æ—¶é—´
	LastFlushLatency time.Duration `json:"last_flush_latency"` // æœ€ååˆ·æ–°å»¶è¿Ÿ
	ChannelLength    int           `json:"channel_length"`     // å½“å‰é€šé“é•¿åº¦
}

// NewArchiveManager åˆ›å»ºå½’æ¡£ç®¡ç†å™¨
func NewArchiveManager(adapter DatabaseAdapter, config ArchiveManagerConfig, pricing map[string]ModelPricing, location *time.Location) *ArchiveManager {
	if config.ChannelSize <= 0 {
		config.ChannelSize = 1000
	}
	if config.BatchSize <= 0 {
		config.BatchSize = 100
	}
	if config.FlushInterval <= 0 {
		config.FlushInterval = 5 * time.Second
	}
	if config.MaxRetry <= 0 {
		config.MaxRetry = 3
	}

	ctx, cancel := context.WithCancel(context.Background())

	am := &ArchiveManager{
		archiveChan: make(chan *ArchiveEvent, config.ChannelSize),
		adapter:     adapter,
		config:      config,
		pricing:     pricing,
		location:    location,
		ctx:         ctx,
		cancel:      cancel,
	}

	// å¯åŠ¨å½’æ¡£åç¨‹
	am.wg.Add(1)
	go am.startArchiver()

	slog.Info("ğŸ“¦ ArchiveManager åˆå§‹åŒ–å®Œæˆ",
		"channel_size", config.ChannelSize,
		"batch_size", config.BatchSize,
		"flush_interval", config.FlushInterval)

	return am
}

// SetHotPool è®¾ç½®çƒ­æ± å¼•ç”¨ï¼ˆç”¨äºå½’æ¡£æˆåŠŸåæ¸…ç†ï¼‰
func (am *ArchiveManager) SetHotPool(hp *HotPool) {
	am.hotPool = hp
}

// UpdateEndpointMultipliers æ›´æ–°ç«¯ç‚¹æˆæœ¬å€ç‡
func (am *ArchiveManager) UpdateEndpointMultipliers(multipliers map[string]EndpointMultiplier) {
	am.endpointMu = multipliers
}

// UpdatePricing æ›´æ–°æ¨¡å‹å®šä»·ï¼ˆè¿è¡Œæ—¶åŠ¨æ€æ›´æ–°ï¼‰
func (am *ArchiveManager) UpdatePricing(pricing map[string]ModelPricing) {
	am.pricing = pricing
}

// Archive å‘é€è¯·æ±‚åˆ°å½’æ¡£é€šé“
func (am *ArchiveManager) Archive(req *ActiveRequest) error {
	event := &ArchiveEvent{
		Request:   req,
		Timestamp: time.Now(),
	}

	// éé˜»å¡å‘é€
	select {
	case am.archiveChan <- event:
		am.stats.mu.Lock()
		am.stats.TotalReceived++
		am.stats.mu.Unlock()
		return nil
	default:
		am.stats.mu.Lock()
		am.stats.TotalDropped++
		am.stats.mu.Unlock()
		slog.Warn("ğŸ“¦ å½’æ¡£é€šé“å·²æ»¡ï¼Œä¸¢å¼ƒäº‹ä»¶",
			"request_id", req.RequestID,
			"channel_length", len(am.archiveChan))
		return fmt.Errorf("archive channel full, event dropped")
	}
}

// startArchiver æ‰¹é‡å½’æ¡£åç¨‹
func (am *ArchiveManager) startArchiver() {
	defer am.wg.Done()

	batch := make([]*ArchiveEvent, 0, am.config.BatchSize)
	ticker := time.NewTicker(am.config.FlushInterval)
	defer ticker.Stop()

	flush := func() {
		if len(batch) == 0 {
			return
		}

		// æå–è¯·æ±‚ ID åˆ—è¡¨
		requestIDs := make([]string, len(batch))
		for i, event := range batch {
			requestIDs[i] = event.Request.RequestID
		}

		start := time.Now()
		err := am.flushBatch(batch)
		latency := time.Since(start)

		am.stats.mu.Lock()
		am.stats.LastFlushTime = time.Now()
		am.stats.LastFlushLatency = latency
		am.stats.TotalBatches++
		if err == nil {
			am.stats.TotalArchived += int64(len(batch))
			// æ›´æ–°å¹³å‡æ‰¹æ¬¡å¤§å°
			am.stats.AvgBatchSize = float64(am.stats.TotalArchived) / float64(am.stats.TotalBatches)
		} else {
			am.stats.TotalFailed += int64(len(batch))
		}
		am.stats.mu.Unlock()

		if err != nil {
			slog.Error("ğŸ“¦ æ‰¹é‡å½’æ¡£å¤±è´¥",
				"batch_size", len(batch),
				"request_ids", strings.Join(requestIDs, ", "),
				"error", err,
				"latency", latency)
		} else {
			slog.Debug("ğŸ“¦ æ‰¹é‡å½’æ¡£æˆåŠŸ",
				"batch_size", len(batch),
				"request_ids", strings.Join(requestIDs, ", "),
				"latency", latency)
		}

		// æ¸…ç©ºæ‰¹æ¬¡
		batch = batch[:0]
	}

	for {
		select {
		case event := <-am.archiveChan:
			batch = append(batch, event)

			// è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³åˆ·æ–°
			if len(batch) >= am.config.BatchSize {
				flush()
			}

		case <-ticker.C:
			// å®šæ—¶åˆ·æ–°ï¼ˆå³ä½¿æœªæ»¡æ‰¹æ¬¡ï¼‰
			flush()

		case <-am.ctx.Done():
			// ä¼˜é›…å…³é—­ï¼Œå¤„ç†å‰©ä½™äº‹ä»¶
			slog.Info("ğŸ“¦ ArchiveManager æ­£åœ¨å…³é—­ï¼Œå¤„ç†å‰©ä½™äº‹ä»¶...")

			// å…ˆåˆ·æ–°å½“å‰æ‰¹æ¬¡
			flush()

			// å¤„ç†é€šé“ä¸­å‰©ä½™çš„äº‹ä»¶
			for {
				select {
				case event := <-am.archiveChan:
					batch = append(batch, event)
					if len(batch) >= am.config.BatchSize {
						flush()
					}
				default:
					// é€šé“å·²ç©ºï¼Œæœ€ååˆ·æ–°
					flush()
					slog.Info("ğŸ“¦ ArchiveManager å·²å…³é—­",
						"total_archived", am.stats.TotalArchived,
						"total_failed", am.stats.TotalFailed)
					return
				}
			}
		}
	}
}

// flushBatch æ‰¹é‡å†™å…¥æ•°æ®åº“
func (am *ArchiveManager) flushBatch(events []*ArchiveEvent) error {
	if len(events) == 0 {
		return nil
	}

	var lastErr error
	for retry := 0; retry < am.config.MaxRetry; retry++ {
		err := am.batchInsert(events)
		if err == nil {
			// å½’æ¡£æˆåŠŸï¼Œä»çƒ­æ± çš„å½’æ¡£ç¼“å­˜ä¸­ç§»é™¤
			if am.hotPool != nil {
				requestIDs := make([]string, len(events))
				for i, event := range events {
					requestIDs[i] = event.Request.RequestID
				}
				am.hotPool.ConfirmArchived(requestIDs)
			}
			return nil
		}

		lastErr = err
		slog.Warn("ğŸ“¦ æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œé‡è¯•ä¸­",
			"retry", retry+1,
			"max_retry", am.config.MaxRetry,
			"error", err)

		// æŒ‡æ•°é€€é¿
		time.Sleep(time.Duration(retry+1) * time.Second)
	}

	return fmt.Errorf("batch insert failed after %d retries: %w", am.config.MaxRetry, lastErr)
}

// batchInsert æ‰§è¡Œæ‰¹é‡æ’å…¥
func (am *ArchiveManager) batchInsert(events []*ArchiveEvent) error {
	if am.adapter == nil {
		return fmt.Errorf("database adapter not initialized")
	}

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	tx, err := am.adapter.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer tx.Rollback()

	// v5.0.1+: æ·»åŠ åˆ†å¼€çš„ 5m/1h ç¼“å­˜å­—æ®µå’Œæˆæœ¬
	stmt, err := tx.PrepareContext(ctx, `
		INSERT INTO request_logs (
			request_id, client_ip, user_agent, method, path,
			start_time, end_time, duration_ms,
			channel, endpoint_name, group_name, model_name,
			status, http_status_code, retry_count,
			failure_reason, cancel_reason,
			is_streaming,
			input_tokens, output_tokens,
			cache_creation_tokens, cache_creation_5m_tokens, cache_creation_1h_tokens,
			cache_read_tokens,
			input_cost_usd, output_cost_usd,
			cache_creation_cost_usd, cache_creation_5m_cost_usd, cache_creation_1h_cost_usd,
			cache_read_cost_usd, total_cost_usd
		) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
	`)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	for _, event := range events {
		req := event.Request

		// v5.0.1+: è®¡ç®—åˆ†å¼€çš„æˆæœ¬
		costBreakdown := am.calculateCostV2(req)

		// æ ¼å¼åŒ–æ—¶é—´
		startTime := am.formatTime(req.StartTime)
		var endTime interface{}
		if req.EndTime != nil {
			endTime = am.formatTime(*req.EndTime)
		} else {
			endTime = nil
		}

		_, err := stmt.ExecContext(ctx,
			req.RequestID,
			req.ClientIP,
			req.UserAgent,
			req.Method,
			req.Path,
			startTime,
			endTime,
			req.DurationMs,
			req.Channel,
			req.EndpointName,
			req.GroupName,
			req.ModelName,
			req.Status,
			req.HTTPStatus,
			req.RetryCount,
			nullString(req.FailureReason),
			nullString(req.CancelReason),
			req.IsStreaming,
			req.InputTokens,
			req.OutputTokens,
			req.CacheCreationTokens,
			req.CacheCreation5mTokens,
			req.CacheCreation1hTokens,
			req.CacheReadTokens,
			costBreakdown.InputCost,
			costBreakdown.OutputCost,
			costBreakdown.CacheCreationCost,   // æ€»æˆæœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
			costBreakdown.CacheCreation5mCost, // 5åˆ†é’Ÿç¼“å­˜æˆæœ¬
			costBreakdown.CacheCreation1hCost, // 1å°æ—¶ç¼“å­˜æˆæœ¬
			costBreakdown.CacheReadCost,
			costBreakdown.TotalCost,
		)
		if err != nil {
			return fmt.Errorf("failed to insert request %s: %w", req.RequestID, err)
		}
	}

	if err := tx.Commit(); err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}

	return nil
}

// calculateCostV2 è®¡ç®—è¯·æ±‚æˆæœ¬ï¼ˆv5.0.1+: æ”¯æŒåˆ†å¼€çš„ 5m/1h ç¼“å­˜ï¼‰
func (am *ArchiveManager) calculateCostV2(req *ActiveRequest) CostBreakdown {
	if am.pricing == nil {
		return CostBreakdown{}
	}

	// æŸ¥æ‰¾æ¨¡å‹å®šä»·ï¼Œä¸å­˜åœ¨åˆ™å›é€€åˆ° _default å®šä»·
	pricing, exists := am.pricing[req.ModelName]
	if !exists {
		pricing, exists = am.pricing["_default"]
		if !exists {
			return CostBreakdown{}
		}
	}

	// è·å–ç«¯ç‚¹å€ç‡
	var multiplier *EndpointMultiplier
	if am.endpointMu != nil {
		if m, ok := am.endpointMu[req.EndpointName]; ok {
			multiplier = &m
		}
	}

	// æ„å»º TokenUsage å¯¹è±¡
	usage := &TokenUsage{
		InputTokens:           req.InputTokens,
		OutputTokens:          req.OutputTokens,
		CacheCreationTokens:   req.CacheCreationTokens,
		CacheCreation5mTokens: req.CacheCreation5mTokens,
		CacheCreation1hTokens: req.CacheCreation1hTokens,
		CacheReadTokens:       req.CacheReadTokens,
	}

	// è°ƒç”¨å…¬å…±æˆæœ¬è®¡ç®—å‡½æ•°ï¼ˆv5.0.1+ï¼‰
	return CalculateCostV2(usage, &pricing, multiplier)
}

// formatTime æ ¼å¼åŒ–æ—¶é—´ä¸ºæ˜“è¯»æ ¼å¼ï¼ˆä½¿ç”¨é…ç½®çš„æ—¶åŒºï¼‰
// æ ¼å¼ï¼š2025-12-04 17:18:48ï¼ˆåŒ—äº¬æ—¶é—´ï¼Œæ— æ—¶åŒºåç¼€ï¼‰
func (am *ArchiveManager) formatTime(t time.Time) string {
	if am.location != nil {
		t = t.In(am.location)
	}
	return t.Format("2006-01-02 15:04:05")
}

// nullString å¤„ç†ç©ºå­—ç¬¦ä¸²ä¸ºSQL NULL
func nullString(s string) interface{} {
	if s == "" {
		return sql.NullString{}
	}
	return s
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆè¿”å›å¿«ç…§ï¼Œä¸å«é”ï¼‰
func (am *ArchiveManager) GetStats() ArchiveStats {
	am.stats.mu.RLock()
	defer am.stats.mu.RUnlock()

	// è¿”å›ä¸å«é”çš„å¿«ç…§å‰¯æœ¬
	return ArchiveStats{
		TotalReceived:    am.stats.TotalReceived,
		TotalArchived:    am.stats.TotalArchived,
		TotalFailed:      am.stats.TotalFailed,
		TotalDropped:     am.stats.TotalDropped,
		TotalBatches:     am.stats.TotalBatches,
		AvgBatchSize:     am.stats.AvgBatchSize,
		LastFlushTime:    am.stats.LastFlushTime,
		LastFlushLatency: am.stats.LastFlushLatency,
		ChannelLength:    len(am.archiveChan),
		// æ³¨æ„ï¼šmu å­—æ®µæ˜¯é›¶å€¼ï¼Œä¸ä¼šå¤åˆ¶åŸå§‹é”
	}
}

// GetChannelLength è·å–å½“å‰é€šé“é•¿åº¦
func (am *ArchiveManager) GetChannelLength() int {
	return len(am.archiveChan)
}

// Close å…³é—­å½’æ¡£ç®¡ç†å™¨
func (am *ArchiveManager) Close() error {
	slog.Info("ğŸ“¦ æ­£åœ¨å…³é—­ ArchiveManager...")
	am.cancel()
	am.wg.Wait()
	return nil
}

// Flush å¼ºåˆ¶åˆ·æ–°ï¼ˆç”¨äºæµ‹è¯•æˆ–ä¼˜é›…å…³é—­ï¼‰
func (am *ArchiveManager) Flush() {
	// å‘é€ä¸€ä¸ªç©ºäº‹ä»¶è§¦å‘åˆ·æ–°
	// å®é™…åˆ·æ–°ç”± ticker å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯ç­‰å¾…
	time.Sleep(100 * time.Millisecond)
}
