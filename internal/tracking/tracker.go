package tracking

import (
	"context"
	"database/sql"
	"embed"
	"encoding/json"
	"fmt"
	"log/slog"
	"os"
	"path/filepath"
	"runtime"
	"sort"
	"sync"
	"time"

	"cc-forwarder/config"
	_ "modernc.org/sqlite"
)

//go:embed schema.sql
var schemaFS embed.FS

// UsageStatsDetailed è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡
type UsageStatsDetailed struct {
	TotalRequests   int64                   `json:"total_requests"`
	SuccessRequests int64                   `json:"success_requests"`
	ErrorRequests   int64                   `json:"error_requests"`
	TotalTokens     int64                   `json:"total_tokens"`
	TotalCost       float64                 `json:"total_cost"`
	ModelStats      map[string]ModelStat    `json:"model_stats"`
	EndpointStats   map[string]EndpointStat `json:"endpoint_stats"`
	GroupStats      map[string]GroupStat    `json:"group_stats"`
}

// ModelStat æ¨¡å‹ç»Ÿè®¡
type ModelStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// EndpointStat ç«¯ç‚¹ç»Ÿè®¡
type EndpointStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// GroupStat ç»„ç»Ÿè®¡
type GroupStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// RequestEvent è¡¨ç¤ºè¯·æ±‚äº‹ä»¶
type RequestEvent struct {
	Type      string      `json:"type"` // "start", "flexible_update", "success", "final_failure", "complete", "failed_request_tokens", "token_recovery"
	RequestID string      `json:"request_id"`
	Timestamp time.Time   `json:"timestamp"`
	Data      interface{} `json:"data"` // æ ¹æ®Typeä¸åŒè€Œå˜åŒ–
}

// RequestStartData è¯·æ±‚å¼€å§‹äº‹ä»¶æ•°æ®
type RequestStartData struct {
	ClientIP    string `json:"client_ip"`
	UserAgent   string `json:"user_agent"`
	Method      string `json:"method"`
	Path        string `json:"path"`
	IsStreaming bool   `json:"is_streaming"` // æ˜¯å¦ä¸ºæµå¼è¯·æ±‚
}

// RequestUpdateData è¯·æ±‚æ›´æ–°äº‹ä»¶æ•°æ®
type RequestUpdateData struct {
	Channel      string `json:"channel"`
	EndpointName string `json:"endpoint_name"`
	GroupName    string `json:"group_name"`
	Status       string `json:"status"`
	RetryCount   int    `json:"retry_count"`
	HTTPStatus   int    `json:"http_status"`
	// ğŸš€ [çŠ¶æ€æœºé‡æ„] Phase 2: æ–°å¢å¤±è´¥åŸå› å’Œå–æ¶ˆåŸå› å­—æ®µ
	FailureReason string `json:"failure_reason,omitempty"`
	CancelReason  string `json:"cancel_reason,omitempty"`
}

// RequestCompleteData è¯·æ±‚å®Œæˆäº‹ä»¶æ•°æ®
type RequestCompleteData struct {
	ModelName             string        `json:"model_name"`
	InputTokens           int64         `json:"input_tokens"`
	OutputTokens          int64         `json:"output_tokens"`
	CacheCreationTokens   int64         `json:"cache_creation_tokens"`
	CacheCreation5mTokens int64         `json:"cache_creation_5m_tokens"` // v5.0.1+
	CacheCreation1hTokens int64         `json:"cache_creation_1h_tokens"` // v5.0.1+
	CacheReadTokens       int64         `json:"cache_read_tokens"`
	Duration              time.Duration `json:"duration"`
	FailureReason         string        `json:"failure_reason,omitempty"` // å¯é€‰ï¼šå¤±è´¥åŸå› 
}

// TokenUsage tokenä½¿ç”¨ç»Ÿè®¡
type TokenUsage struct {
	InputTokens           int64
	OutputTokens          int64
	CacheCreationTokens   int64 // æ€»ç¼“å­˜åˆ›å»º tokensï¼ˆå‘åå…¼å®¹ï¼Œ= 5m + 1hï¼‰
	CacheReadTokens       int64
	CacheCreation5mTokens int64 // 5åˆ†é’Ÿç¼“å­˜åˆ›å»º tokens (1.25x å®šä»·)
	CacheCreation1hTokens int64 // 1å°æ—¶ç¼“å­˜åˆ›å»º tokens (2x å®šä»·)
}

// ModelPricing æ¨¡å‹å®šä»·é…ç½®
type ModelPricing struct {
	Input           float64 `yaml:"input"`             // per 1M tokens
	Output          float64 `yaml:"output"`            // per 1M tokens
	CacheCreation   float64 `yaml:"cache_creation"`    // per 1M tokens (5åˆ†é’Ÿç¼“å­˜åˆ›å»ºï¼Œ1.25x input)
	CacheCreation1h float64 `yaml:"cache_creation_1h"` // per 1M tokens (1å°æ—¶ç¼“å­˜åˆ›å»ºï¼Œ2x input)
	CacheRead       float64 `yaml:"cache_read"`        // per 1M tokens (ç¼“å­˜è¯»å–)
}

// EndpointMultiplier ç«¯ç‚¹æˆæœ¬å€ç‡ï¼ˆv5.0+ æ”¯æŒç«¯ç‚¹çº§åˆ«çš„æˆæœ¬è°ƒæ•´ï¼‰
// æˆæœ¬è®¡ç®—å…¬å¼ï¼šæ¨¡å‹åŸºç¡€å®šä»· * ç«¯ç‚¹å€ç‡
type EndpointMultiplier struct {
	CostMultiplier                float64 // æ€»ä½“å€ç‡ï¼ˆå¦‚æœ > 0ï¼Œä¼šè¦†ç›–åˆ†é¡¹å€ç‡ï¼‰
	InputCostMultiplier           float64 // è¾“å…¥æˆæœ¬å€ç‡
	OutputCostMultiplier          float64 // è¾“å‡ºæˆæœ¬å€ç‡
	CacheCreationCostMultiplier   float64 // 5åˆ†é’Ÿç¼“å­˜åˆ›å»ºæˆæœ¬å€ç‡
	CacheCreationCostMultiplier1h float64 // 1å°æ—¶ç¼“å­˜åˆ›å»ºæˆæœ¬å€ç‡
	CacheReadCostMultiplier       float64 // ç¼“å­˜è¯»å–æˆæœ¬å€ç‡
}

// DefaultEndpointMultiplier è¿”å›é»˜è®¤ç«¯ç‚¹å€ç‡ï¼ˆå…¨éƒ¨ä¸º 1.0ï¼‰
func DefaultEndpointMultiplier() EndpointMultiplier {
	return EndpointMultiplier{
		CostMultiplier:                1.0,
		InputCostMultiplier:           1.0,
		OutputCostMultiplier:          1.0,
		CacheCreationCostMultiplier:   1.0,
		CacheCreationCostMultiplier1h: 1.0,
		CacheReadCostMultiplier:       1.0,
	}
}

// CostBreakdown æˆæœ¬åˆ†è§£ç»“æ„
type CostBreakdown struct {
	InputCost           float64
	OutputCost          float64
	CacheCreationCost   float64 // æ€»ç¼“å­˜åˆ›å»ºæˆæœ¬ (5m + 1h)
	CacheCreation5mCost float64 // 5åˆ†é’Ÿç¼“å­˜åˆ›å»ºæˆæœ¬
	CacheCreation1hCost float64 // 1å°æ—¶ç¼“å­˜åˆ›å»ºæˆæœ¬
	CacheReadCost       float64
	TotalCost           float64
}

// CalculateCostV2 ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å‡½æ•°ï¼ˆv5.0+ æ”¯æŒåˆ†å¼€çš„ç¼“å­˜å®šä»·ï¼‰
// æ¶ˆé™¤ archive_manager.go å’Œ tracker.go ä¸­çš„é‡å¤ä»£ç 
//
// å‚æ•°:
//   - usage: Token ä½¿ç”¨é‡ï¼ˆåŒ…å«åˆ†å¼€çš„ 5m/1h ç¼“å­˜ tokensï¼‰
//   - pricing: æ¨¡å‹å®šä»·ï¼Œnil æ—¶è¿”å›é›¶æˆæœ¬
//   - multiplier: ç«¯ç‚¹å€ç‡ï¼Œnil æ—¶ä½¿ç”¨é»˜è®¤å€ç‡ 1.0
func CalculateCostV2(usage *TokenUsage, pricing *ModelPricing, multiplier *EndpointMultiplier) CostBreakdown {
	if pricing == nil || usage == nil {
		return CostBreakdown{}
	}

	// ä½¿ç”¨é»˜è®¤å€ç‡
	var m EndpointMultiplier
	if multiplier != nil {
		m = *multiplier
	} else {
		m = DefaultEndpointMultiplier()
	}

	// ç¡®ä¿å€ç‡æœ‰æ•ˆï¼ˆ0 æˆ–è´Ÿæ•°è§†ä¸º 1.0ï¼‰
	ensureMultiplier := func(v float64) float64 {
		if v <= 0 {
			return 1.0
		}
		return v
	}

	// è·å– 1h ç¼“å­˜å®šä»·ï¼ˆå¦‚æœæœªè®¾ç½®ï¼Œä½¿ç”¨ 2x input ä½œä¸ºé»˜è®¤ï¼‰
	cacheCreation1hPrice := pricing.CacheCreation1h
	if cacheCreation1hPrice <= 0 {
		cacheCreation1hPrice = pricing.Input * 2.0 // é»˜è®¤ 2x input
	}

	// åŸºç¡€æˆæœ¬è®¡ç®—ï¼ˆæ¯ç™¾ä¸‡ tokenï¼‰
	inputCost := float64(usage.InputTokens) * pricing.Input / 1_000_000
	outputCost := float64(usage.OutputTokens) * pricing.Output / 1_000_000
	cacheReadCost := float64(usage.CacheReadTokens) * pricing.CacheRead / 1_000_000

	// åˆ†å¼€è®¡ç®— 5m å’Œ 1h ç¼“å­˜æˆæœ¬
	var cache5mCost, cache1hCost float64
	if usage.CacheCreation5mTokens > 0 || usage.CacheCreation1hTokens > 0 {
		// æœ‰åˆ†å¼€çš„ç¼“å­˜æ•°æ®ï¼Œåˆ†åˆ«è®¡ç®—
		cache5mCost = float64(usage.CacheCreation5mTokens) * pricing.CacheCreation / 1_000_000
		cache1hCost = float64(usage.CacheCreation1hTokens) * cacheCreation1hPrice / 1_000_000
	} else {
		// å‘åå…¼å®¹ï¼šåªæœ‰æ€»æ•°ï¼Œé»˜è®¤ä½¿ç”¨ 5m å®šä»·
		cache5mCost = float64(usage.CacheCreationTokens) * pricing.CacheCreation / 1_000_000
		cache1hCost = 0
	}
	cacheCreationCost := cache5mCost + cache1hCost

	// åº”ç”¨ç«¯ç‚¹å€ç‡
	if m.CostMultiplier > 0 {
		// æ€»ä½“å€ç‡æ¨¡å¼ï¼šæ‰€æœ‰æˆæœ¬ç»Ÿä¸€ä¹˜ä»¥æ€»ä½“å€ç‡
		totalMultiplier := ensureMultiplier(m.CostMultiplier)
		inputCost *= totalMultiplier
		outputCost *= totalMultiplier
		cache5mCost *= totalMultiplier
		cache1hCost *= totalMultiplier
		cacheCreationCost *= totalMultiplier
		cacheReadCost *= totalMultiplier
	} else {
		// åˆ†é¡¹å€ç‡æ¨¡å¼ï¼šå„é¡¹æˆæœ¬åˆ†åˆ«ä¹˜ä»¥å¯¹åº”å€ç‡
		inputCost *= ensureMultiplier(m.InputCostMultiplier)
		outputCost *= ensureMultiplier(m.OutputCostMultiplier)
		cacheReadCost *= ensureMultiplier(m.CacheReadCostMultiplier)
		cache5mCost *= ensureMultiplier(m.CacheCreationCostMultiplier)
		cache1hCost *= ensureMultiplier(m.CacheCreationCostMultiplier1h)
		cacheCreationCost = cache5mCost + cache1hCost
	}

	return CostBreakdown{
		InputCost:           inputCost,
		OutputCost:          outputCost,
		CacheCreationCost:   cacheCreationCost,
		CacheCreation5mCost: cache5mCost,
		CacheCreation1hCost: cache1hCost,
		CacheReadCost:       cacheReadCost,
		TotalCost:           inputCost + outputCost + cacheCreationCost + cacheReadCost,
	}
}

// CalculateCost ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
// Deprecated: ä½¿ç”¨ CalculateCostV2 ä»¥æ”¯æŒåˆ†å¼€çš„ç¼“å­˜å®šä»·
//
// å‚æ•°:
//   - inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens: Token ä½¿ç”¨é‡
//   - pricing: æ¨¡å‹å®šä»·ï¼Œnil æ—¶è¿”å›é›¶æˆæœ¬
//   - multiplier: ç«¯ç‚¹å€ç‡ï¼Œnil æ—¶ä½¿ç”¨é»˜è®¤å€ç‡ 1.0
//   - use1hCache: æ˜¯å¦ä½¿ç”¨ 1 å°æ—¶ç¼“å­˜å€ç‡ï¼ˆç”¨äºé•¿æ•ˆç¼“å­˜åœºæ™¯ï¼‰
func CalculateCost(inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64,
	pricing *ModelPricing, multiplier *EndpointMultiplier, use1hCache bool) CostBreakdown {

	// æ„å»º TokenUsageï¼Œæ ¹æ® use1hCache å†³å®šæ”¾å…¥å“ªä¸ªå­—æ®µ
	usage := &TokenUsage{
		InputTokens:     inputTokens,
		OutputTokens:    outputTokens,
		CacheReadTokens: cacheReadTokens,
	}
	if use1hCache {
		usage.CacheCreation1hTokens = cacheCreationTokens
	} else {
		usage.CacheCreation5mTokens = cacheCreationTokens
	}
	usage.CacheCreationTokens = cacheCreationTokens // ä¿æŒæ€»æ•°

	return CalculateCostV2(usage, pricing, multiplier)
}

// Config ä½¿ç”¨è·Ÿè¸ªé…ç½®
type Config struct {
	Enabled bool `yaml:"enabled"`

	// å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰çš„ database_path é…ç½®
	DatabasePath string `yaml:"database_path"`

	// æ–°å¢ï¼šæ•°æ®åº“é…ç½®ï¼ˆä¼˜å…ˆçº§é«˜äº DatabasePathï¼‰
	Database *config.DatabaseBackendConfig `yaml:"database,omitempty"`

	BufferSize      int                     `yaml:"buffer_size"`
	BatchSize       int                     `yaml:"batch_size"`
	FlushInterval   time.Duration           `yaml:"flush_interval"`
	MaxRetry        int                     `yaml:"max_retry"`
	RetentionDays   int                     `yaml:"retention_days"`
	CleanupInterval time.Duration           `yaml:"cleanup_interval"`
	ModelPricing    map[string]ModelPricing `yaml:"model_pricing"`
	DefaultPricing  ModelPricing            `yaml:"default_pricing"`

	// ğŸ”¥ v4.1 æ–°å¢ï¼šçƒ­æ± é…ç½®
	HotPool *HotPoolSettings `yaml:"hot_pool,omitempty"`
}

// HotPoolSettings çƒ­æ± é…ç½®
type HotPoolSettings struct {
	Enabled          bool          `yaml:"enabled"`            // æ˜¯å¦å¯ç”¨çƒ­æ± æ¨¡å¼ï¼ˆé»˜è®¤trueï¼‰
	MaxAge           time.Duration `yaml:"max_age"`            // æœ€å¤§å­˜æ´»æ—¶é—´ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
	MaxSize          int           `yaml:"max_size"`           // æœ€å¤§å®¹é‡ï¼ˆé»˜è®¤10000ï¼‰
	CleanupInterval  time.Duration `yaml:"cleanup_interval"`   // æ¸…ç†é—´éš”ï¼ˆé»˜è®¤1åˆ†é’Ÿï¼‰
	ArchiveOnCleanup bool          `yaml:"archive_on_cleanup"` // æ¸…ç†æ—¶æ˜¯å¦å½’æ¡£ï¼ˆé»˜è®¤trueï¼‰
}

// WriteRequest å†™æ“ä½œè¯·æ±‚
type WriteRequest struct {
	Query     string
	Args      []interface{}
	Response  chan error
	Context   context.Context
	EventType string // ç”¨äºè°ƒè¯•å’Œç›‘æ§
}

// UpdateOptions ç»Ÿä¸€çš„è¯·æ±‚æ›´æ–°é€‰é¡¹
// æ”¯æŒå¯é€‰å­—æ®µæ›´æ–°ï¼Œåªæ›´æ–°énilçš„å­—æ®µ
type UpdateOptions struct {
	EndpointName  *string        // ç«¯ç‚¹åç§°
	Channel       *string        // æ¸ é“æ ‡ç­¾ï¼ˆv5.0ï¼‰
	GroupName     *string        // ç»„åç§°
	AuthType      *string        // è®¤è¯ç±»å‹ï¼štoken/api_key
	AuthKey       *string        // è®¤è¯æ ‡è¯†ï¼ˆè„±æ•/æŒ‡çº¹ï¼‰ï¼Œä¸å«æ˜æ–‡
	Status        *string        // çŠ¶æ€
	RetryCount    *int           // é‡è¯•æ¬¡æ•°
	HttpStatus    *int           // HTTPçŠ¶æ€ç 
	ModelName     *string        // æ¨¡å‹åç§°
	EndTime       *time.Time     // ç»“æŸæ—¶é—´
	Duration      *time.Duration // æŒç»­æ—¶é—´
	FailureReason *string        // å¤±è´¥åŸå› ï¼ˆç”¨äºä¸­é—´è¿‡ç¨‹è®°å½•ï¼‰
}

// UsageTracker ä½¿ç”¨è·Ÿè¸ªå™¨
type UsageTracker struct {
	// åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
	db           *sql.DB
	eventChan    chan RequestEvent
	config       *Config
	pricing      map[string]ModelPricing       // æ¨¡å‹å®šä»·ç¼“å­˜
	endpointMu   map[string]EndpointMultiplier // ç«¯ç‚¹å€ç‡ç¼“å­˜
	ctx          context.Context
	cancel       context.CancelFunc
	wg           sync.WaitGroup
	mu           sync.RWMutex
	errorHandler *ErrorHandler

	// æ—¶åŒºæ”¯æŒ
	location *time.Location // é…ç½®çš„æ—¶åŒº

	// æ–°å¢ï¼šæ•°æ®åº“é€‚é…å™¨
	adapter DatabaseAdapter // æ•°æ®åº“é€‚é…å™¨æ¥å£

	// æ–°å¢ï¼šè¯»å†™åˆ†ç¦»ç»„ä»¶ï¼ˆä»é€‚é…å™¨è·å–ï¼‰
	readDB     *sql.DB           // è¯»è¿æ¥æ±  (å¤šè¿æ¥)
	writeDB    *sql.DB           // å†™è¿æ¥ (å•è¿æ¥)
	writeQueue chan WriteRequest // å†™æ“ä½œé˜Ÿåˆ—
	writeMu    sync.Mutex        // å†™æ“ä½œä¿æŠ¤é”
	writeWg    sync.WaitGroup    // å†™å¤„ç†å™¨ç­‰å¾…ç»„

	// ğŸ”¥ v4.1 æ–°å¢ï¼šå†…å­˜çƒ­æ±  + å½’æ¡£æ¶æ„
	hotPool        *HotPool        // å†…å­˜çƒ­æ± ï¼ˆæ´»è·ƒè¯·æ±‚ï¼‰
	archiveManager *ArchiveManager // å½’æ¡£ç®¡ç†å™¨ï¼ˆæ‰¹é‡å†™å…¥ï¼‰
	hotPoolEnabled bool            // æ˜¯å¦å¯ç”¨çƒ­æ± æ¨¡å¼
}

// NewUsageTracker åˆ›å»ºæ–°çš„ä½¿ç”¨è·Ÿè¸ªå™¨
func NewUsageTracker(config *Config, globalTimezone ...string) (*UsageTracker, error) {
	if config == nil || !config.Enabled {
		return &UsageTracker{config: config}, nil
	}

	// è®¾ç½®é»˜è®¤å€¼
	if config.BufferSize <= 0 {
		config.BufferSize = 1000
	}
	if config.BatchSize <= 0 {
		config.BatchSize = 100
	}
	if config.FlushInterval <= 0 {
		config.FlushInterval = 30 * time.Second
	}
	if config.MaxRetry <= 0 {
		config.MaxRetry = 3
	}
	if config.CleanupInterval <= 0 {
		config.CleanupInterval = 24 * time.Hour // é»˜è®¤24å°æ—¶æ¸…ç†ä¸€æ¬¡
	}

	// æ„å»ºæ•°æ®åº“é…ç½®
	tz := ""
	if len(globalTimezone) > 0 {
		tz = globalTimezone[0]
	}
	dbConfig, err := buildDatabaseConfig(config, tz)
	if err != nil {
		return nil, fmt.Errorf("failed to build database config: %w", err)
	}

	// åˆ›å»ºæ•°æ®åº“é€‚é…å™¨
	adapter, err := NewDatabaseAdapter(dbConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create database adapter: %w", err)
	}

	// æ‰“å¼€æ•°æ®åº“è¿æ¥
	if err := adapter.Open(); err != nil {
		return nil, fmt.Errorf("failed to open database: %w", err)
	}

	// è·å–è¯»å†™è¿æ¥ï¼ˆä»é€‚é…å™¨ï¼‰
	readDB := adapter.GetReadDB()
	writeDB := adapter.GetWriteDB()

	// ä¿æŒåŸæœ‰dbå­—æ®µå…¼å®¹æ€§ï¼ˆæŒ‡å‘readDBï¼‰
	db := readDB

	ctx, cancel := context.WithCancel(context.Background())

	// åˆå§‹åŒ–æ—¶åŒº
	timezone := dbConfig.Timezone
	if timezone == "" {
		timezone = "Asia/Shanghai" // é»˜è®¤æ—¶åŒº
	}
	location, err := time.LoadLocation(timezone)
	if err != nil {
		slog.Warn("åŠ è½½æ—¶åŒºå¤±è´¥ï¼Œä½¿ç”¨Asia/Shanghai", "timezone", timezone, "error", err)
		location, _ = time.LoadLocation("Asia/Shanghai")
		if location == nil {
			location = time.FixedZone("CST", 8*3600) // åå¤‡æ–¹æ¡ˆï¼šå›ºå®š+8æ—¶åŒº
		}
	}

	ut := &UsageTracker{
		// åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
		db:        db, // å…¼å®¹æ€§ï¼šæŒ‡å‘readDB
		eventChan: make(chan RequestEvent, config.BufferSize),
		config:    config,
		pricing:   config.ModelPricing,
		ctx:       ctx,
		cancel:    cancel,

		// æ—¶åŒºæ”¯æŒ
		location: location,

		// æ–°å¢ï¼šæ•°æ®åº“é€‚é…å™¨
		adapter: adapter,

		// è¯»å†™åˆ†ç¦»ç»„ä»¶ï¼ˆä»é€‚é…å™¨è·å–ï¼‰
		readDB:     readDB,
		writeDB:    writeDB,
		writeQueue: make(chan WriteRequest, config.BufferSize), // ä¸äº‹ä»¶é˜Ÿåˆ—å®¹é‡ä¸€è‡´
	}

	// åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
	ut.errorHandler = NewErrorHandler(ut, slog.Default())

	// åˆå§‹åŒ–æ•°æ®åº“Schemaï¼ˆä½¿ç”¨é€‚é…å™¨ï¼‰
	if err := ut.initDatabaseWithAdapter(); err != nil {
		cancel()
		adapter.Close()
		return nil, fmt.Errorf("failed to initialize database: %w", err)
	}

	// å¯åŠ¨å†™æ“ä½œå¤„ç†å™¨
	go ut.processWriteQueue()

	// å¯åŠ¨å¼‚æ­¥äº‹ä»¶å¤„ç†å™¨
	ut.wg.Add(1)
	go ut.processEvents()

	// å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
	ut.wg.Add(1)
	go ut.periodicCleanup()

	// å¯åŠ¨å®šæœŸå¤‡ä»½ä»»åŠ¡
	ut.wg.Add(1)
	go ut.periodicBackup()

	// ğŸ”¥ v4.1 åˆå§‹åŒ–çƒ­æ± æ¶æ„
	ut.initHotPool()

	slog.Info("âœ… ä½¿ç”¨è·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆ",
		"database_type", adapter.GetDatabaseType(),
		"buffer_size", config.BufferSize,
		"batch_size", config.BatchSize,
		"hot_pool_enabled", ut.hotPoolEnabled)

	return ut, nil
}

// initHotPool åˆå§‹åŒ–çƒ­æ± å’Œå½’æ¡£ç®¡ç†å™¨
func (ut *UsageTracker) initHotPool() {
	// æ£€æŸ¥æ˜¯å¦å¯ç”¨çƒ­æ± æ¨¡å¼ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
	hotPoolEnabled := true
	var hotPoolConfig HotPoolConfig

	if ut.config.HotPool != nil {
		// ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
		if !ut.config.HotPool.Enabled {
			hotPoolEnabled = false
		}
		hotPoolConfig = HotPoolConfig{
			MaxAge:           ut.config.HotPool.MaxAge,
			MaxSize:          ut.config.HotPool.MaxSize,
			CleanupInterval:  ut.config.HotPool.CleanupInterval,
			ArchiveOnCleanup: ut.config.HotPool.ArchiveOnCleanup,
		}
	} else {
		// ä½¿ç”¨é»˜è®¤é…ç½®
		hotPoolConfig = DefaultHotPoolConfig()
	}

	if !hotPoolEnabled {
		slog.Info("ğŸ”¥ çƒ­æ± æ¨¡å¼å·²ç¦ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿäº‹ä»¶é˜Ÿåˆ—æ¨¡å¼")
		ut.hotPoolEnabled = false
		return
	}

	// åˆ›å»ºçƒ­æ± 
	ut.hotPool = NewHotPool(hotPoolConfig)

	// åˆ›å»ºå½’æ¡£ç®¡ç†å™¨
	archiveConfig := ArchiveManagerConfig{
		ChannelSize:   ut.config.BufferSize,
		BatchSize:     ut.config.BatchSize,
		FlushInterval: ut.config.FlushInterval,
		MaxRetry:      ut.config.MaxRetry,
	}
	ut.archiveManager = NewArchiveManager(ut.adapter, archiveConfig, ut.pricing, ut.location)

	// è®¾ç½®åŒå‘å¼•ç”¨ï¼šArchiveManager éœ€è¦è®¿é—® HotPool æ¥æ¸…ç†å½’æ¡£ç¼“å­˜
	ut.archiveManager.SetHotPool(ut.hotPool)

	// è®¾ç½®çƒ­æ± å½’æ¡£å›è°ƒ
	ut.hotPool.SetArchiveCallback(func(req *ActiveRequest) {
		if ut.archiveManager != nil {
			ut.archiveManager.Archive(req)
		}
	})

	ut.hotPoolEnabled = true
	slog.Info("ğŸ”¥ çƒ­æ± æ¨¡å¼å·²å¯ç”¨",
		"max_age", hotPoolConfig.MaxAge,
		"max_size", hotPoolConfig.MaxSize)
}

// now è¿”å›å½“å‰é…ç½®æ—¶åŒºçš„æ—¶é—´
func (ut *UsageTracker) now() time.Time {
	if ut.location == nil {
		return time.Now() // åå¤‡æ–¹æ¡ˆ
	}
	return time.Now().In(ut.location)
}

// buildDatabaseConfig ä»Configæ„å»ºDatabaseConfig
// v4.1.0: ç®€åŒ–ä¸ºä»…æ”¯æŒ SQLite
func buildDatabaseConfig(config *Config, globalTimezone string) (DatabaseConfig, error) {
	var dbConfig DatabaseConfig

	// è®¾ç½®æ•°æ®åº“ç±»å‹ï¼ˆv4.1+ ä»…æ”¯æŒ SQLiteï¼‰
	dbConfig.Type = "sqlite"

	// ä¼˜å…ˆä½¿ç”¨æ–°çš„Databaseé…ç½®
	if config.Database != nil {
		// æ£€æŸ¥æ˜¯å¦é…ç½®äº† MySQLï¼ˆå·²åºŸå¼ƒï¼‰
		if config.Database.Type == "mysql" {
			slog.Warn("âš ï¸  MySQL é…ç½®å·²åºŸå¼ƒï¼Œå°†ä½¿ç”¨ SQLite",
				"reason", "v4.1.0 ç§»é™¤äº† MySQL æ”¯æŒ",
				"suggestion", "è¯·ä¿®æ”¹é…ç½® type: sqlite æˆ–åˆ é™¤ database.type é…ç½®")
		}
		// å…¼å®¹ï¼šé…ç½®é‡Œå†™äº† database ä½†æ²¡å†™ path æ—¶ï¼Œå›é€€åˆ° DatabasePathï¼ˆç”±ä¸Šå±‚é»˜è®¤å¡«å……ä¸ºç”¨æˆ·ç›®å½•è·¯å¾„ï¼‰ã€‚
		if config.Database.Path != "" {
			dbConfig.DatabasePath = config.Database.Path
		} else {
			dbConfig.DatabasePath = config.DatabasePath
		}
		if config.Database.Timezone != "" {
			dbConfig.Timezone = config.Database.Timezone
		} else {
			dbConfig.Timezone = globalTimezone
		}
	} else {
		// å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰çš„DatabasePathé…ç½®
		dbConfig.DatabasePath = config.DatabasePath
	}

	// è®¾ç½®é»˜è®¤æ•°æ®åº“è·¯å¾„ - ä½¿ç”¨è·¨å¹³å°ç”¨æˆ·ç›®å½•
	if dbConfig.DatabasePath == "" {
		// ä½¿ç”¨ internal/utils æä¾›çš„è·¨å¹³å°ç›®å½•
		// Windows: %APPDATA%\CC-Forwarder\data\cc-forwarder.db
		// macOS: ~/Library/Application Support/CC-Forwarder/data/cc-forwarder.db
		// Linux: ~/.local/share/cc-forwarder/data/cc-forwarder.db
		dbConfig.DatabasePath = filepath.Join(getAppDataDir(), "data", "cc-forwarder.db")
	}

	// æ—¶åŒºçº§è”é€»è¾‘ï¼šä¼˜å…ˆçº§ database.timezone > global.timezone > é»˜è®¤å€¼
	if dbConfig.Timezone == "" {
		if globalTimezone != "" {
			dbConfig.Timezone = globalTimezone
		}
		// setDefaultConfig ä¼šè®¾ç½®é»˜è®¤å€¼ Asia/Shanghai
	}

	return dbConfig, nil
}

// getAppDataDir è·å–åº”ç”¨æ•°æ®ç›®å½•ï¼ˆè·¨å¹³å°ï¼‰
// å¤åˆ¶è‡ª internal/utils/appdir.goï¼Œé¿å…å¾ªç¯ä¾èµ–
// Windows: %APPDATA%\CC-Forwarder
// macOS: ~/Library/Application Support/CC-Forwarder
// Linux: ~/.local/share/cc-forwarder
func getAppDataDir() string {
	var baseDir string

	switch runtime.GOOS {
	case "windows":
		baseDir = os.Getenv("APPDATA")
		if baseDir == "" {
			baseDir = filepath.Join(os.Getenv("USERPROFILE"), "AppData", "Roaming")
		}
		return filepath.Join(baseDir, "CC-Forwarder")

	case "darwin":
		homeDir, _ := os.UserHomeDir()
		return filepath.Join(homeDir, "Library", "Application Support", "CC-Forwarder")

	case "linux":
		homeDir, _ := os.UserHomeDir()
		xdgDataHome := os.Getenv("XDG_DATA_HOME")
		if xdgDataHome != "" {
			return filepath.Join(xdgDataHome, "cc-forwarder")
		}
		return filepath.Join(homeDir, ".local", "share", "cc-forwarder")

	default:
		homeDir, _ := os.UserHomeDir()
		return filepath.Join(homeDir, ".cc-forwarder")
	}
}

// initDatabaseWithAdapter ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–æ•°æ®åº“
func (ut *UsageTracker) initDatabaseWithAdapter() error {
	if ut.adapter == nil {
		return fmt.Errorf("database adapter not initialized")
	}

	// ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–Schema
	if err := ut.adapter.InitSchema(); err != nil {
		return fmt.Errorf("failed to initialize database schema: %w", err)
	}

	slog.Info("æ•°æ®åº“Schemaåˆå§‹åŒ–å®Œæˆ",
		"database_type", ut.adapter.GetDatabaseType())

	return nil
}

// Close å…³é—­ä½¿ç”¨è·Ÿè¸ªå™¨
func (ut *UsageTracker) Close() error {
	if ut.config == nil || !ut.config.Enabled {
		return nil
	}

	// å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å…³é—­
	ut.mu.RLock()
	if ut.cancel == nil {
		ut.mu.RUnlock()
		return nil // å·²ç»å…³é—­è¿‡
	}
	ut.mu.RUnlock()

	slog.Info("Shutting down usage tracker...")

	// å–æ¶ˆä¸Šä¸‹æ–‡ï¼ˆä¸éœ€è¦æŒæœ‰é”ï¼‰
	ut.cancel()

	// ç­‰å¾…æ‰€æœ‰åç¨‹å®Œæˆï¼ˆåŒ…æ‹¬å†™å¤„ç†å™¨ï¼‰
	ut.wg.Wait()
	ut.writeWg.Wait() // ç­‰å¾…å†™å¤„ç†å™¨å®Œæˆ

	// ç°åœ¨å¯ä»¥å®‰å…¨åœ°æŒæœ‰å†™é”è¿›è¡Œæ¸…ç†
	ut.mu.Lock()
	defer ut.mu.Unlock()

	ut.cancel = nil // æ ‡è®°ä¸ºå·²å…³é—­

	// å…³é—­äº‹ä»¶é€šé“
	if ut.eventChan != nil {
		close(ut.eventChan)
		ut.eventChan = nil
	}

	// å…³é—­å†™æ“ä½œé˜Ÿåˆ—
	if ut.writeQueue != nil {
		close(ut.writeQueue)
		ut.writeQueue = nil
	}

	// ğŸ”¥ v4.1 å…³é—­çƒ­æ± å’Œå½’æ¡£ç®¡ç†å™¨
	if ut.hotPool != nil {
		if err := ut.hotPool.Close(); err != nil {
			slog.Error("Failed to close hot pool", "error", err)
		}
		ut.hotPool = nil
	}
	if ut.archiveManager != nil {
		if err := ut.archiveManager.Close(); err != nil {
			slog.Error("Failed to close archive manager", "error", err)
		}
		ut.archiveManager = nil
	}

	// å…³é—­æ•°æ®åº“é€‚é…å™¨ï¼ˆä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¿æ¥ï¼‰
	if ut.adapter != nil {
		if err := ut.adapter.Close(); err != nil {
			slog.Error("Failed to close database adapter", "error", err)
			return fmt.Errorf("failed to close database adapter: %w", err)
		}
		ut.adapter = nil
	}

	// æ¸…ç†è¿æ¥å¼•ç”¨ï¼ˆè¿™äº›ç°åœ¨ç”±adapterç®¡ç†ï¼‰
	ut.readDB = nil
	ut.writeDB = nil
	ut.db = nil

	slog.Info("âœ… ä½¿ç”¨è·Ÿè¸ªå™¨å…³é—­å®Œæˆ")
	return nil
}

// RecordRequestStart è®°å½•è¯·æ±‚å¼€å§‹
func (ut *UsageTracker) RecordRequestStart(requestID, clientIP, userAgent, method, path string, isStreaming bool) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	// ğŸ”¥ v4.1 çƒ­æ± æ¨¡å¼ï¼šç›´æ¥æ·»åŠ åˆ°å†…å­˜çƒ­æ± 
	if ut.hotPoolEnabled && ut.hotPool != nil {
		req := NewActiveRequest(requestID, clientIP, userAgent, method, path, isStreaming)
		if err := ut.hotPool.Add(req); err != nil {
			slog.Warn("ğŸ”¥ çƒ­æ± æ·»åŠ è¯·æ±‚å¤±è´¥ï¼Œé™çº§åˆ°äº‹ä»¶é˜Ÿåˆ—æ¨¡å¼",
				"request_id", requestID,
				"error", err)
			// é™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
			ut.recordRequestStartLegacy(requestID, clientIP, userAgent, method, path, isStreaming)
		}
		return
	}

	// ä¼ ç»Ÿæ¨¡å¼ï¼šå‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—
	ut.recordRequestStartLegacy(requestID, clientIP, userAgent, method, path, isStreaming)
}

// recordRequestStartLegacy ä¼ ç»Ÿæ¨¡å¼è®°å½•è¯·æ±‚å¼€å§‹
func (ut *UsageTracker) recordRequestStartLegacy(requestID, clientIP, userAgent, method, path string, isStreaming bool) {
	event := RequestEvent{
		Type:      "start",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestStartData{
			ClientIP:    clientIP,
			UserAgent:   userAgent,
			Method:      method,
			Path:        path,
			IsStreaming: isStreaming,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		// ç¼“å†²åŒºæ»¡æ—¶çš„å¤„ç†ç­–ç•¥
		slog.Warn("Usage tracking event buffer full, dropping start event",
			"request_id", requestID)
	}
}

// RecordRequestUpdate ç»Ÿä¸€çš„è¯·æ±‚æ›´æ–°æ–¹æ³•
// æ”¯æŒå¯é€‰å­—æ®µæ›´æ–°ï¼Œåªæ›´æ–°énilçš„å­—æ®µï¼Œé€‚ç”¨äºæ‰€æœ‰ä¸­é—´è¿‡ç¨‹çŠ¶æ€å˜æ›´
func (ut *UsageTracker) RecordRequestUpdate(requestID string, opts UpdateOptions) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	// ğŸ”¥ v4.1 çƒ­æ± æ¨¡å¼ï¼šç›´æ¥æ›´æ–°å†…å­˜ï¼ˆæ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼šæ— æ•°æ®åº“å†™å…¥ï¼‰
	if ut.hotPoolEnabled && ut.hotPool != nil {
		err := ut.hotPool.Update(requestID, func(req *ActiveRequest) {
			// åªæ›´æ–°énilçš„å­—æ®µ
			if opts.EndpointName != nil {
				req.EndpointName = *opts.EndpointName
			}
			if opts.Channel != nil {
				req.Channel = *opts.Channel
			}
			if opts.GroupName != nil {
				req.GroupName = *opts.GroupName
			}
			if opts.AuthType != nil {
				req.AuthType = *opts.AuthType
			}
			if opts.AuthKey != nil {
				req.AuthKey = *opts.AuthKey
			}
			if opts.Status != nil {
				req.Status = *opts.Status
			}
			if opts.RetryCount != nil {
				req.RetryCount = *opts.RetryCount
			}
			if opts.HttpStatus != nil {
				req.HTTPStatus = *opts.HttpStatus
			}
			if opts.ModelName != nil {
				req.ModelName = *opts.ModelName
			}
			if opts.FailureReason != nil {
				req.FailureReason = *opts.FailureReason
			}
		})
		if err != nil {
			// è¯·æ±‚å¯èƒ½ä¸åœ¨çƒ­æ± ä¸­ï¼ˆå·²å½’æ¡£æˆ–ä»æœªè®°å½•ï¼‰ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
			slog.Debug("ğŸ”¥ çƒ­æ± æ›´æ–°è¯·æ±‚å¤±è´¥ï¼Œé™çº§åˆ°äº‹ä»¶é˜Ÿåˆ—æ¨¡å¼",
				"request_id", requestID,
				"error", err)
			ut.recordRequestUpdateLegacy(requestID, opts)
		}
		return
	}

	// ä¼ ç»Ÿæ¨¡å¼ï¼šå‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—
	ut.recordRequestUpdateLegacy(requestID, opts)
}

// recordRequestUpdateLegacy ä¼ ç»Ÿæ¨¡å¼æ›´æ–°è¯·æ±‚
func (ut *UsageTracker) recordRequestUpdateLegacy(requestID string, opts UpdateOptions) {
	event := RequestEvent{
		Type:      "flexible_update",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data:      opts,
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping flexible_update event",
			"request_id", requestID)
	}
}

// RecordRequestSuccess è®°å½•è¯·æ±‚æˆåŠŸå®Œæˆ
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æˆåŠŸç›¸å…³å­—æ®µï¼šstatus='completed', end_time, duration_ms, Tokenå’Œæˆæœ¬ä¿¡æ¯
func (ut *UsageTracker) RecordRequestSuccess(requestID, modelName string, tokens *TokenUsage, duration time.Duration) {
	ut.RecordRequestSuccessWithQuality(requestID, modelName, tokens, duration, "")
}

// RecordRequestSuccessWithQuality è®°å½•è¯·æ±‚æˆåŠŸå®Œæˆï¼ˆæ”¯æŒæ•°æ®è´¨é‡æ ‡è®°ï¼‰
// ğŸ”§ [æ–¹æ¡ˆAå®ç°] 2025-12-20: åŸå­æ“ä½œï¼Œåœ¨ CompleteAndArchive ä¸­ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰å­—æ®µåŒ…æ‹¬ failureReason
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æˆåŠŸç›¸å…³å­—æ®µï¼šstatus='completed', end_time, duration_ms, Tokenã€æˆæœ¬ä¿¡æ¯å’Œå¯é€‰çš„ failure_reason
func (ut *UsageTracker) RecordRequestSuccessWithQuality(requestID, modelName string, tokens *TokenUsage, duration time.Duration, failureReason string) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	// ğŸš€ [æ¶æ„ä¿®å¤] æ”¯æŒ nil tokensï¼Œç¡®ä¿è€—æ—¶ä¿¡æ¯æ€»æ˜¯è¢«è®°å½•
	var inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64
	var cacheCreation5mTokens, cacheCreation1hTokens int64 // v5.0.1+
	if tokens != nil {
		inputTokens = tokens.InputTokens
		outputTokens = tokens.OutputTokens
		cacheCreationTokens = tokens.CacheCreationTokens
		cacheCreation5mTokens = tokens.CacheCreation5mTokens
		cacheCreation1hTokens = tokens.CacheCreation1hTokens
		cacheReadTokens = tokens.CacheReadTokens

		// ğŸ”§ [å‘åå…¼å®¹ä¿®å¤] v5.0.1+: å¦‚æœ 5m/1h éƒ½æ˜¯ 0 ä½†æ€»æ•°ä¸ä¸º 0ï¼Œ
		// å°†æ€»æ•°åˆ†é…ç»™ 5m å­—æ®µï¼Œä¸æˆæœ¬è®¡ç®—é€»è¾‘ä¿æŒä¸€è‡´ï¼ˆtracker.go:185-193ï¼‰
		if cacheCreation5mTokens == 0 && cacheCreation1hTokens == 0 && cacheCreationTokens > 0 {
			cacheCreation5mTokens = cacheCreationTokens // é»˜è®¤æŒ‰ 5m ç¼“å­˜å¤„ç†
			slog.Debug("ğŸ”„ [ç¼“å­˜Tokenåˆ†é…] å‘åå…¼å®¹æ¨¡å¼ï¼šå°†æ€»ç¼“å­˜Tokenåˆ†é…åˆ°5må­—æ®µ",
				"request_id", requestID,
				"cache_creation_tokens", cacheCreationTokens)
		}
	}
	// å¦‚æœ tokens ä¸º nilï¼Œæ‰€æœ‰ token å­—æ®µéƒ½æ˜¯ 0ï¼Œä½† duration ä»ç„¶ä¼šè¢«è®°å½•

	// ğŸ”¥ v4.1 çƒ­æ± æ¨¡å¼ï¼šå®Œæˆè¯·æ±‚å¹¶å½’æ¡£ï¼ˆæ ¸å¿ƒï¼šå•æ¬¡æ•°æ®åº“å†™å…¥ï¼‰
	if ut.hotPoolEnabled && ut.hotPool != nil {
		now := ut.now()
		err := ut.hotPool.CompleteAndArchive(requestID, func(req *ActiveRequest) {
			req.Status = "completed"
			req.ModelName = modelName
			req.InputTokens = inputTokens
			req.OutputTokens = outputTokens
			req.CacheCreationTokens = cacheCreationTokens
			req.CacheCreation5mTokens = cacheCreation5mTokens // v5.0.1+
			req.CacheCreation1hTokens = cacheCreation1hTokens // v5.0.1+
			req.CacheReadTokens = cacheReadTokens
			req.EndTime = &now
			req.DurationMs = duration.Milliseconds()
			// ğŸ”§ [æ–¹æ¡ˆAå®ç°] 2025-12-20: æ˜¾å¼è¦†ç›– failureReasonï¼ˆæ— è®ºæ˜¯å¦ä¸ºç©ºï¼‰
			// é¿å…ä¹‹å‰ä¸­é€”é”™è¯¯è®¾ç½®çš„æ—§å€¼æ®‹ç•™ï¼Œå¯¼è‡´"æˆåŠŸä½†å¸¦å¤±è´¥åŸå› "çš„è¯¯æ ‡
			req.FailureReason = failureReason
			// æˆæœ¬åœ¨å½’æ¡£æ—¶è®¡ç®—
		})
		if err != nil {
			// è¯·æ±‚å¯èƒ½ä¸åœ¨çƒ­æ± ä¸­ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
			slog.Debug("ğŸ”¥ çƒ­æ± å®Œæˆè¯·æ±‚å¤±è´¥ï¼Œé™çº§åˆ°äº‹ä»¶é˜Ÿåˆ—æ¨¡å¼",
				"request_id", requestID,
				"error", err)
			ut.recordRequestSuccessLegacy(requestID, modelName, inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens, duration, failureReason)
		}
		return
	}

	// ä¼ ç»Ÿæ¨¡å¼ï¼šå‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—
	ut.recordRequestSuccessLegacy(requestID, modelName, inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens, duration, failureReason)
}

// recordRequestSuccessLegacy ä¼ ç»Ÿæ¨¡å¼è®°å½•è¯·æ±‚æˆåŠŸ
// ğŸ”§ [æ–¹æ¡ˆAå®ç°] 2025-12-20: å¢åŠ  failureReason å‚æ•°æ”¯æŒ
func (ut *UsageTracker) recordRequestSuccessLegacy(requestID, modelName string, inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64, duration time.Duration, failureReason string) {
	event := RequestEvent{
		Type:      "success",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestCompleteData{
			ModelName:           modelName,
			InputTokens:         inputTokens,
			OutputTokens:        outputTokens,
			CacheCreationTokens: cacheCreationTokens,
			CacheReadTokens:     cacheReadTokens,
			Duration:            duration,
			FailureReason:       failureReason,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping success event",
			"request_id", requestID)
	}
}

// RecordRequestFinalFailure è®°å½•è¯·æ±‚æœ€ç»ˆå¤±è´¥æˆ–å–æ¶ˆ
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰å¤±è´¥/å–æ¶ˆç›¸å…³å­—æ®µï¼šstatus, end_time, duration_ms, failure_reason/cancel_reason, http_status_code, å¯é€‰Token
// ğŸ”§ [ä¿®å¤] 2025-12-11: æ·»åŠ  modelName å‚æ•°ï¼Œç¡®ä¿å–æ¶ˆ/å¤±è´¥è¯·æ±‚èƒ½æ­£ç¡®è®¡ç®—æˆæœ¬
func (ut *UsageTracker) RecordRequestFinalFailure(requestID, modelName, status, reason, errorDetail string, duration time.Duration, httpStatus int, tokens *TokenUsage) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	// å¤„ç†Tokenä¿¡æ¯ï¼ˆå¤±è´¥/å–æ¶ˆæ—¶å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰ï¼‰
	var inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64
	var cacheCreation5mTokens, cacheCreation1hTokens int64 // ğŸ”§ [ä¿®å¤] 2025-12-11: æ·»åŠ  5m/1h ç¼“å­˜å­—æ®µ
	if tokens != nil {
		inputTokens = tokens.InputTokens
		outputTokens = tokens.OutputTokens
		cacheCreationTokens = tokens.CacheCreationTokens
		cacheCreation5mTokens = tokens.CacheCreation5mTokens
		cacheCreation1hTokens = tokens.CacheCreation1hTokens
		cacheReadTokens = tokens.CacheReadTokens

		// ğŸ”§ [å‘åå…¼å®¹ä¿®å¤] v5.0.1+: å¦‚æœ 5m/1h éƒ½æ˜¯ 0 ä½†æ€»æ•°ä¸ä¸º 0ï¼Œ
		// å°†æ€»æ•°åˆ†é…ç»™ 5m å­—æ®µï¼Œä¸æˆæœ¬è®¡ç®—é€»è¾‘ä¿æŒä¸€è‡´
		if cacheCreation5mTokens == 0 && cacheCreation1hTokens == 0 && cacheCreationTokens > 0 {
			cacheCreation5mTokens = cacheCreationTokens
		}
	}

	// ğŸ”¥ v4.1 çƒ­æ± æ¨¡å¼ï¼šå®Œæˆå¤±è´¥è¯·æ±‚å¹¶å½’æ¡£
	if ut.hotPoolEnabled && ut.hotPool != nil {
		now := ut.now()
		err := ut.hotPool.CompleteAndArchive(requestID, func(req *ActiveRequest) {
			req.Status = status
			req.HTTPStatus = httpStatus
			// ğŸ”§ [ä¿®å¤] 2025-12-11: è®¾ç½®æ¨¡å‹åï¼Œå¦åˆ™æˆæœ¬è®¡ç®—ä¼šå¤±è´¥
			if modelName != "" && modelName != "unknown" {
				req.ModelName = modelName
			}
			// ğŸ”§ [ä¿®å¤] 2025-12-15: åªæœ‰å½“ tokens å‚æ•°ä¸ä¸º nil æ—¶æ‰æ›´æ–° token å­—æ®µ
			// å¦åˆ™ä¿ç•™çƒ­æ± ä¸­å·²æœ‰çš„å€¼ï¼ˆå¯èƒ½ç”± RecordFailedRequestTokens è®¾ç½®ï¼‰
			if tokens != nil {
				req.InputTokens = inputTokens
				req.OutputTokens = outputTokens
				req.CacheCreationTokens = cacheCreationTokens
				req.CacheCreation5mTokens = cacheCreation5mTokens
				req.CacheCreation1hTokens = cacheCreation1hTokens
				req.CacheReadTokens = cacheReadTokens
			}
			req.EndTime = &now
			req.DurationMs = duration.Milliseconds()
			// æ ¹æ®çŠ¶æ€è®¾ç½®åŸå› å­—æ®µ
			if status == "cancelled" {
				req.CancelReason = reason
			} else {
				req.FailureReason = reason
				if errorDetail != "" {
					req.FailureReason = reason + ": " + errorDetail
				}
			}
		})
		if err != nil {
			// è¯·æ±‚å¯èƒ½ä¸åœ¨çƒ­æ± ä¸­ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
			slog.Debug("ğŸ”¥ çƒ­æ± å®Œæˆå¤±è´¥è¯·æ±‚å¤±è´¥ï¼Œé™çº§åˆ°äº‹ä»¶é˜Ÿåˆ—æ¨¡å¼",
				"request_id", requestID,
				"error", err)
			ut.recordRequestFinalFailureLegacy(requestID, modelName, status, reason, errorDetail, duration, httpStatus, inputTokens, outputTokens, cacheCreationTokens, cacheCreation5mTokens, cacheCreation1hTokens, cacheReadTokens)
		}
		return
	}

	// ä¼ ç»Ÿæ¨¡å¼ï¼šå‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—
	ut.recordRequestFinalFailureLegacy(requestID, modelName, status, reason, errorDetail, duration, httpStatus, inputTokens, outputTokens, cacheCreationTokens, cacheCreation5mTokens, cacheCreation1hTokens, cacheReadTokens)
}

// recordRequestFinalFailureLegacy ä¼ ç»Ÿæ¨¡å¼è®°å½•è¯·æ±‚å¤±è´¥
// ğŸ”§ [ä¿®å¤] 2025-12-11: æ·»åŠ  modelName å’Œ 5m/1h ç¼“å­˜å­—æ®µå‚æ•°
func (ut *UsageTracker) recordRequestFinalFailureLegacy(requestID, modelName, status, reason, errorDetail string, duration time.Duration, httpStatus int, inputTokens, outputTokens, cacheCreationTokens, cacheCreation5mTokens, cacheCreation1hTokens, cacheReadTokens int64) {
	event := RequestEvent{
		Type:      "final_failure",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: map[string]interface{}{
			"status":                   status, // "failed" or "cancelled"
			"reason":                   reason, // failure_reason or cancel_reason
			"error_detail":             errorDetail,
			"duration":                 duration,
			"http_status":              httpStatus, // HTTPçŠ¶æ€ç 
			"model_name":               modelName,  // ğŸ”§ [ä¿®å¤] 2025-12-11: æ·»åŠ æ¨¡å‹å
			"input_tokens":             inputTokens,
			"output_tokens":            outputTokens,
			"cache_creation_tokens":    cacheCreationTokens,
			"cache_creation_5m_tokens": cacheCreation5mTokens, // ğŸ”§ [ä¿®å¤] 2025-12-11
			"cache_creation_1h_tokens": cacheCreation1hTokens, // ğŸ”§ [ä¿®å¤] 2025-12-11
			"cache_read_tokens":        cacheReadTokens,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping final_failure event",
			"request_id", requestID)
	}
}

// RecordFailedRequestTokens è®°å½•å¤±è´¥è¯·æ±‚çš„Tokenä½¿ç”¨
// åªè®°å½•Tokenç»Ÿè®¡ï¼Œä¸å½±å“è¯·æ±‚çŠ¶æ€
func (ut *UsageTracker) RecordFailedRequestTokens(requestID, modelName string, tokens *TokenUsage, duration time.Duration, failureReason string) {
	if ut.config == nil || !ut.config.Enabled || tokens == nil {
		return
	}

	// ğŸ”¥ v4.1 çƒ­æ± æ¨¡å¼ï¼šç›´æ¥æ›´æ–°å†…å­˜ä¸­çš„è¯·æ±‚æ•°æ®
	// è¿™æ ·åç»­çš„ CompleteAndArchive ä¼šå°†æ­£ç¡®çš„ token ä¿¡æ¯å†™å…¥æ•°æ®åº“
	if ut.hotPoolEnabled && ut.hotPool != nil {
		err := ut.hotPool.Update(requestID, func(req *ActiveRequest) {
			// æ›´æ–° Token ä¿¡æ¯
			req.InputTokens = tokens.InputTokens
			req.OutputTokens = tokens.OutputTokens
			req.CacheCreationTokens = tokens.CacheCreationTokens
			req.CacheCreation5mTokens = tokens.CacheCreation5mTokens
			req.CacheCreation1hTokens = tokens.CacheCreation1hTokens
			req.CacheReadTokens = tokens.CacheReadTokens
			// æ›´æ–°æ¨¡å‹åï¼ˆå¦‚æœæœ‰ï¼‰
			if modelName != "" && modelName != "unknown" {
				req.ModelName = modelName
			}
			// æ›´æ–°æŒç»­æ—¶é—´
			if duration > 0 {
				req.DurationMs = duration.Milliseconds()
			}
			// è®°å½•å¤±è´¥åŸå› ï¼ˆä¸æ”¹å˜çŠ¶æ€ï¼‰
			if failureReason != "" && req.FailureReason == "" {
				req.FailureReason = failureReason
			}
		})
		if err == nil {
			slog.Debug(fmt.Sprintf("ğŸ”¥ [çƒ­æ± Tokenæ›´æ–°] [%s] åŸå› : %s, æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d",
				requestID, failureReason, modelName, tokens.InputTokens, tokens.OutputTokens))
			return
		}
		// è¯·æ±‚ä¸åœ¨çƒ­æ± ä¸­ï¼ˆå¯èƒ½å·²å½’æ¡£ï¼‰ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
		slog.Debug(fmt.Sprintf("ğŸ”¥ [çƒ­æ± Tokenæ›´æ–°å¤±è´¥] [%s] é™çº§åˆ°äº‹ä»¶é˜Ÿåˆ—æ¨¡å¼, é”™è¯¯: %v",
			requestID, err))
	}

	// ä¼ ç»Ÿæ¨¡å¼ï¼šå‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—ï¼ˆUPDATE å·²å­˜åœ¨çš„æ•°æ®åº“è®°å½•ï¼‰
	event := RequestEvent{
		Type:      "failed_request_tokens",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestCompleteData{
			ModelName:             modelName,
			InputTokens:           tokens.InputTokens,
			OutputTokens:          tokens.OutputTokens,
			CacheCreationTokens:   tokens.CacheCreationTokens,
			CacheCreation5mTokens: tokens.CacheCreation5mTokens,
			CacheCreation1hTokens: tokens.CacheCreation1hTokens,
			CacheReadTokens:       tokens.CacheReadTokens,
			Duration:              duration,
			FailureReason:         failureReason,
		},
	}

	select {
	case ut.eventChan <- event:
		slog.Debug(fmt.Sprintf("ğŸ’¾ [å¤±è´¥Tokenäº‹ä»¶] [%s] åŸå› : %s, æ¨¡å‹: %s", requestID, failureReason, modelName))
	default:
		slog.Warn("Usage tracking event buffer full, dropping failed request tokens event",
			"request_id", requestID)
	}
}

// RecoverRequestTokens æ¢å¤è¯·æ±‚çš„Tokenä½¿ç”¨ç»Ÿè®¡
// ğŸ”§ [Fallbackä¿®å¤] ä¸“ç”¨äºdebugæ–‡ä»¶æ¢å¤åœºæ™¯ï¼Œä»…æ›´æ–°Tokenå­—æ®µï¼Œä¸è§¦å‘çŠ¶æ€å˜æ›´
func (ut *UsageTracker) RecoverRequestTokens(requestID, modelName string, tokens *TokenUsage) {
	if ut.config == nil || !ut.config.Enabled || tokens == nil {
		return
	}

	// ğŸ”¥ v4.1 çƒ­æ± æ¨¡å¼ï¼šå¦‚æœè¯·æ±‚è¿˜åœ¨çƒ­æ± ä¸­ï¼Œå…ˆæ›´æ–°çƒ­æ± 
	if ut.hotPoolEnabled && ut.hotPool != nil {
		err := ut.hotPool.Update(requestID, func(req *ActiveRequest) {
			// æ›´æ–° Token ä¿¡æ¯
			req.InputTokens = tokens.InputTokens
			req.OutputTokens = tokens.OutputTokens
			req.CacheCreationTokens = tokens.CacheCreationTokens
			req.CacheCreation5mTokens = tokens.CacheCreation5mTokens
			req.CacheCreation1hTokens = tokens.CacheCreation1hTokens
			req.CacheReadTokens = tokens.CacheReadTokens
			// æ›´æ–°æ¨¡å‹åï¼ˆå¦‚æœæœ‰ï¼‰
			if modelName != "" && modelName != "unknown" {
				req.ModelName = modelName
			}
		})
		if err == nil {
			slog.Info(fmt.Sprintf("ğŸ”¥ [çƒ­æ± Tokenæ¢å¤] [%s] æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d",
				requestID, modelName, tokens.InputTokens, tokens.OutputTokens))
			return
		}
		// è¯·æ±‚ä¸åœ¨çƒ­æ± ä¸­ï¼ˆå·²å½’æ¡£ï¼‰ï¼Œç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼æ›´æ–°æ•°æ®åº“
		slog.Debug(fmt.Sprintf("ğŸ”¥ [çƒ­æ± Tokenæ¢å¤å¤±è´¥] [%s] è¯·æ±‚å·²å½’æ¡£ï¼Œä½¿ç”¨UPDATEæ›´æ–°æ•°æ®åº“",
			requestID))
	}

	// ä¼ ç»Ÿæ¨¡å¼ï¼šå‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—ï¼ˆUPDATE å·²å­˜åœ¨çš„æ•°æ®åº“è®°å½•ï¼‰
	event := RequestEvent{
		Type:      "token_recovery",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestCompleteData{
			ModelName:             modelName,
			InputTokens:           tokens.InputTokens,
			OutputTokens:          tokens.OutputTokens,
			CacheCreationTokens:   tokens.CacheCreationTokens,
			CacheCreation5mTokens: tokens.CacheCreation5mTokens,
			CacheCreation1hTokens: tokens.CacheCreation1hTokens,
			CacheReadTokens:       tokens.CacheReadTokens,
			Duration:              0, // ä¸æ›´æ–°æ—¶é—´ç›¸å…³å­—æ®µ
		},
	}

	select {
	case ut.eventChan <- event:
		slog.Info(fmt.Sprintf("ğŸ”§ [Tokenæ¢å¤äº‹ä»¶] [%s] æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d",
			requestID, modelName, tokens.InputTokens, tokens.OutputTokens))
	default:
		slog.Warn("Usage tracking event buffer full, dropping token recovery event",
			"request_id", requestID)
	}
}

// UpdatePricing æ›´æ–°æ¨¡å‹å®šä»·ï¼ˆè¿è¡Œæ—¶åŠ¨æ€æ›´æ–°ï¼‰
func (ut *UsageTracker) UpdatePricing(pricing map[string]ModelPricing) {
	ut.mu.Lock()
	defer ut.mu.Unlock()

	ut.pricing = pricing

	// åŒæ­¥åˆ° ArchiveManager
	if ut.archiveManager != nil {
		ut.archiveManager.UpdatePricing(pricing)
	}

	slog.Info("Model pricing updated", "model_count", len(pricing))
}

// UpdateEndpointMultipliers æ›´æ–°ç«¯ç‚¹æˆæœ¬å€ç‡
// æˆæœ¬è®¡ç®—å…¬å¼ï¼šæ¨¡å‹åŸºç¡€å®šä»· * ç«¯ç‚¹å€ç‡
func (ut *UsageTracker) UpdateEndpointMultipliers(multipliers map[string]EndpointMultiplier) {
	ut.mu.Lock()
	defer ut.mu.Unlock()

	ut.endpointMu = multipliers

	// åŒæ­¥åˆ° ArchiveManager
	if ut.archiveManager != nil {
		ut.archiveManager.UpdateEndpointMultipliers(multipliers)
	}

	slog.Info("Endpoint multipliers updated", "endpoint_count", len(multipliers))
}

// GetEndpointMultiplier è·å–ç«¯ç‚¹å€ç‡ï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
func (ut *UsageTracker) GetEndpointMultiplier(endpointName string) EndpointMultiplier {
	ut.mu.RLock()
	defer ut.mu.RUnlock()

	if ut.endpointMu == nil {
		return EndpointMultiplier{
			CostMultiplier:                1.0,
			InputCostMultiplier:           1.0,
			OutputCostMultiplier:          1.0,
			CacheCreationCostMultiplier:   1.0,
			CacheCreationCostMultiplier1h: 1.0,
			CacheReadCostMultiplier:       1.0,
		}
	}

	if m, exists := ut.endpointMu[endpointName]; exists {
		return m
	}

	// è¿”å›é»˜è®¤å€ç‡ 1.0
	return EndpointMultiplier{
		CostMultiplier:                1.0,
		InputCostMultiplier:           1.0,
		OutputCostMultiplier:          1.0,
		CacheCreationCostMultiplier:   1.0,
		CacheCreationCostMultiplier1h: 1.0,
		CacheReadCostMultiplier:       1.0,
	}
}

// GetDatabaseStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…è£…æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetDatabaseStats(ctx context.Context) (*DatabaseStats, error) {
	if ut.config == nil || !ut.config.Enabled {
		return nil, fmt.Errorf("usage tracking not enabled")
	}
	if ut.db == nil {
		return nil, fmt.Errorf("database not initialized")
	}
	return ut.getDatabaseStatsInternal(ctx)
}

// HealthCheck æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€å’ŒåŸºæœ¬åŠŸèƒ½ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) HealthCheck(ctx context.Context) error {
	if ut.config == nil || !ut.config.Enabled {
		return nil // å¦‚æœæœªå¯ç”¨ï¼Œè®¤ä¸ºæ˜¯å¥åº·çš„
	}

	if ut.readDB == nil {
		return fmt.Errorf("read database not initialized")
	}

	// æµ‹è¯•è¯»æ•°æ®åº“è¿æ¥
	if err := ut.readDB.PingContext(ctx); err != nil {
		return fmt.Errorf("read database ping failed: %w", err)
	}

	// æµ‹è¯•å†™æ•°æ®åº“è¿æ¥
	if ut.writeDB != nil {
		if err := ut.writeDB.PingContext(ctx); err != nil {
			return fmt.Errorf("write database ping failed: %w", err)
		}
	}

	// æµ‹è¯•åŸºæœ¬æŸ¥è¯¢ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	var count int
	err := ut.readDB.QueryRowContext(ctx, "SELECT COUNT(*) FROM sqlite_master WHERE type='table'").Scan(&count)
	if err != nil {
		return fmt.Errorf("database query test failed: %w", err)
	}

	// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
	if count < 2 { // è‡³å°‘åº”è¯¥æœ‰ request_logs å’Œ usage_summary ä¸¤ä¸ªè¡¨
		return fmt.Errorf("database schema incomplete: expected at least 2 tables, found %d", count)
	}

	// æ£€æŸ¥äº‹ä»¶å¤„ç†é€šé“æ˜¯å¦æ­£å¸¸
	select {
	case <-ut.ctx.Done():
		return fmt.Errorf("usage tracker context cancelled")
	default:
		// ä¸Šä¸‹æ–‡æ­£å¸¸
	}

	// æ£€æŸ¥äº‹ä»¶é€šé“å®¹é‡
	if ut.eventChan != nil {
		channelLoad := float64(len(ut.eventChan)) / float64(cap(ut.eventChan)) * 100
		if channelLoad > 90 {
			return fmt.Errorf("event channel overloaded: %.1f%% capacity used", channelLoad)
		}
	}

	// æ£€æŸ¥å†™é˜Ÿåˆ—å®¹é‡
	if ut.writeQueue != nil {
		writeQueueLoad := float64(len(ut.writeQueue)) / float64(cap(ut.writeQueue)) * 100
		if writeQueueLoad > 90 {
			return fmt.Errorf("write queue overloaded: %.1f%% capacity used", writeQueueLoad)
		}
	}

	return nil
}

// ForceFlush å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰å¾…å¤„ç†äº‹ä»¶
func (ut *UsageTracker) ForceFlush() error {
	if ut.config == nil || !ut.config.Enabled {
		return nil
	}

	// å°è¯•å‘é€ä¸€ä¸ªç‰¹æ®Šäº‹ä»¶æ¥è§¦å‘æ‰¹å¤„ç†
	flushEvent := RequestEvent{
		Type:      "flush",
		RequestID: "force-flush-" + ut.now().Format("20060102150405"),
		Timestamp: ut.now(),
		Data:      nil,
	}

	select {
	case ut.eventChan <- flushEvent:
		slog.Info("Force flush event sent")
		return nil
	default:
		return fmt.Errorf("event channel full, cannot force flush")
	}
}

// GetPricing è·å–æ¨¡å‹å®šä»·
func (ut *UsageTracker) GetPricing(modelName string) ModelPricing {
	ut.mu.RLock()
	defer ut.mu.RUnlock()

	if pricing, exists := ut.pricing[modelName]; exists {
		return pricing
	}
	return ut.config.DefaultPricing
}

// GetConfiguredModels è·å–é…ç½®ä¸­çš„æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
func (ut *UsageTracker) GetConfiguredModels() []string {
	ut.mu.RLock()
	defer ut.mu.RUnlock()

	models := make([]string, 0, len(ut.pricing))
	for modelName := range ut.pricing {
		models = append(models, modelName)
	}

	return models
}

// GetUsageSummary è·å–ä½¿ç”¨æ‘˜è¦ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetUsageSummary(ctx context.Context, startTime, endTime time.Time) ([]UsageSummary, error) {
	opts := &QueryOptions{
		StartDate: &startTime,
		EndDate:   &endTime,
		Limit:     100,
	}
	return ut.QueryUsageSummary(ctx, opts)
}

// GetRequestLogs è·å–è¯·æ±‚æ—¥å¿—ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetRequestLogs(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string, limit, offset int) ([]RequestDetail, error) {
	opts := &QueryOptions{
		StartDate:    &startTime,
		EndDate:      &endTime,
		ModelName:    modelName,
		EndpointName: endpointName,
		GroupName:    groupName,
		Limit:        limit,
		Offset:       offset,
	}
	return ut.QueryRequestDetails(ctx, opts)
}

// GetUsageStats è·å–ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼Œä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) GetUsageStats(ctx context.Context, startTime, endTime time.Time) (*UsageStatsDetailed, error) {
	if ut.readDB == nil {
		return nil, fmt.Errorf("read database not initialized")
	}

	query := `SELECT 
		COUNT(*) as total_requests,
		SUM(CASE WHEN status IN ('completed', 'processing') THEN 1 ELSE 0 END) as success_requests,
		SUM(CASE WHEN status IN ('failed', 'error', 'auth_error', 'rate_limited', 'server_error', 'network_error', 'stream_error', 'timeout') THEN 1 ELSE 0 END) as error_requests,
		SUM(input_tokens + output_tokens + cache_creation_tokens + cache_read_tokens) as total_tokens,
		SUM(total_cost_usd) as total_cost
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ?`

	var stats UsageStatsDetailed
	err := ut.readDB.QueryRowContext(ctx, query, startTime, endTime).Scan(
		&stats.TotalRequests,
		&stats.SuccessRequests,
		&stats.ErrorRequests,
		&stats.TotalTokens,
		&stats.TotalCost,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to query detailed usage stats: %w", err)
	}

	// è·å–æ¨¡å‹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	modelQuery := `SELECT model_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND model_name IS NOT NULL AND model_name != ''
		GROUP BY model_name`

	rows, err := ut.readDB.QueryContext(ctx, modelQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query model stats: %w", err)
	}
	defer rows.Close()

	stats.ModelStats = make(map[string]ModelStat)
	for rows.Next() {
		var modelName string
		var requests int64
		var cost float64
		if err := rows.Scan(&modelName, &requests, &cost); err != nil {
			continue
		}
		stats.ModelStats[modelName] = ModelStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}

	// è·å–ç«¯ç‚¹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	endpointQuery := `SELECT endpoint_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND endpoint_name IS NOT NULL AND endpoint_name != ''
		GROUP BY endpoint_name`

	rows2, err := ut.readDB.QueryContext(ctx, endpointQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query endpoint stats: %w", err)
	}
	defer rows2.Close()

	stats.EndpointStats = make(map[string]EndpointStat)
	for rows2.Next() {
		var endpointName string
		var requests int64
		var cost float64
		if err := rows2.Scan(&endpointName, &requests, &cost); err != nil {
			continue
		}
		stats.EndpointStats[endpointName] = EndpointStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}

	// è·å–ç»„ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	groupQuery := `SELECT group_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND group_name IS NOT NULL AND group_name != ''
		GROUP BY group_name`

	rows3, err := ut.readDB.QueryContext(ctx, groupQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query group stats: %w", err)
	}
	defer rows3.Close()

	stats.GroupStats = make(map[string]GroupStat)
	for rows3.Next() {
		var groupName string
		var requests int64
		var cost float64
		if err := rows3.Scan(&groupName, &requests, &cost); err != nil {
			continue
		}
		stats.GroupStats[groupName] = GroupStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}

	return &stats, nil
}

// ExportToCSV å¯¼å‡ºä¸ºCSVæ ¼å¼
func (ut *UsageTracker) ExportToCSV(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string) ([]byte, error) {
	logs, err := ut.GetRequestLogs(ctx, startTime, endTime, modelName, endpointName, groupName, 10000, 0) // Export up to 10k records
	if err != nil {
		return nil, fmt.Errorf("failed to get request logs for CSV export: %w", err)
	}

	// CSV header
	csv := "request_id,client_ip,user_agent,method,path,start_time,end_time,duration_ms,channel,endpoint_name,group_name,model_name,auth_type,auth_key,status,http_status_code,retry_count,input_tokens,output_tokens,cache_creation_tokens,cache_read_tokens,input_cost_usd,output_cost_usd,cache_creation_cost_usd,cache_read_cost_usd,total_cost_usd,created_at,updated_at\n"

	// CSV rows
	for _, log := range logs {
		endTime := ""
		if log.EndTime != nil {
			endTime = log.EndTime.Format(time.RFC3339)
		}

		durationMs := ""
		if log.DurationMs != nil {
			durationMs = fmt.Sprintf("%d", *log.DurationMs)
		}

		httpStatus := ""
		if log.HTTPStatusCode != nil {
			httpStatus = fmt.Sprintf("%d", *log.HTTPStatusCode)
		}

		csv += fmt.Sprintf("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%d,%d,%d,%d,%d,%.6f,%.6f,%.6f,%.6f,%.6f,%s,%s\n",
			log.RequestID, log.ClientIP, log.UserAgent, log.Method, log.Path,
			log.StartTime.Format(time.RFC3339), endTime, durationMs,
			log.Channel, log.EndpointName, log.GroupName, log.ModelName, log.AuthType, log.AuthKey, log.Status,
			httpStatus, log.RetryCount,
			log.InputTokens, log.OutputTokens, log.CacheCreationTokens, log.CacheReadTokens,
			log.InputCostUSD, log.OutputCostUSD, log.CacheCreationCostUSD, log.CacheReadCostUSD, log.TotalCostUSD,
			log.CreatedAt.Format(time.RFC3339), log.UpdatedAt.Format(time.RFC3339),
		)
	}

	return []byte(csv), nil
}

// ExportToJSON å¯¼å‡ºä¸ºJSONæ ¼å¼
func (ut *UsageTracker) ExportToJSON(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string) ([]byte, error) {
	logs, err := ut.GetRequestLogs(ctx, startTime, endTime, modelName, endpointName, groupName, 10000, 0) // Export up to 10k records
	if err != nil {
		return nil, fmt.Errorf("failed to get request logs for JSON export: %w", err)
	}

	// ä½¿ç”¨æ ‡å‡†åº“çš„jsonåŒ…åºåˆ—åŒ–
	jsonBytes, err := json.Marshal(logs)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal logs to JSON: %w", err)
	}

	return jsonBytes, nil
}

// processWriteQueue å¯åŠ¨å†™æ“ä½œé˜Ÿåˆ—å¤„ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼Œç¡®ä¿ç¨³å®šæ€§ï¼‰
func (ut *UsageTracker) processWriteQueue() {
	ut.writeWg.Add(1)
	defer ut.writeWg.Done()
	slog.Debug("Write processor started")

	for {
		select {
		case writeReq := <-ut.writeQueue:
			err := ut.executeWriteSimple(writeReq)
			writeReq.Response <- err

		case <-ut.ctx.Done():
			// é€€å‡ºå‰ï¼šå°½é‡å”¤é†’æ‰€æœ‰ç­‰å¾…ä¸­çš„å†™è¯·æ±‚ï¼Œé¿å…å…³é—­æ—¶å‡ºç° goroutine å¡æ­»
			drainErr := ut.ctx.Err()
			for {
				select {
				case writeReq := <-ut.writeQueue:
					writeReq.Response <- drainErr
				default:
					slog.Debug("Write processor stopped")
					return
				}
			}
		}
	}
}

// executeWriteSimple æ‰§è¡Œç®€å•å†™æ“ä½œï¼ˆé¿å…å¤æ‚çš„æ‰¹å¤„ç†ï¼‰
func (ut *UsageTracker) executeWriteSimple(req WriteRequest) error {
	ut.writeMu.Lock()
	defer ut.writeMu.Unlock()

	ctx, cancel := context.WithTimeout(req.Context, 30*time.Second)
	defer cancel()

	// ç›´æ¥æ‰§è¡Œï¼Œä¸ä½¿ç”¨äº‹åŠ¡ï¼ˆå¯¹äºç®€å•INSERT/UPDATEï¼Œä¸ä¸€å®šéœ€è¦äº‹åŠ¡ï¼‰
	if req.EventType == "vacuum" {
		// VACUUMä¸èƒ½åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œ
		_, err := ut.writeDB.ExecContext(ctx, req.Query, req.Args...)
		return err
	}

	// å…¶ä»–æ“ä½œä½¿ç”¨çŸ­äº‹åŠ¡
	tx, err := ut.writeDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}

	committed := false
	defer func() {
		if !committed {
			if rbErr := tx.Rollback(); rbErr != nil {
				slog.Debug("Failed to rollback transaction", "error", rbErr, "event_type", req.EventType)
			}
		}
	}()

	result, err := tx.ExecContext(ctx, req.Query, req.Args...)
	if err != nil {
		return fmt.Errorf("failed to execute query: %w", err)
	}

	// å¯¹äºå¿…é¡»å‘½ä¸­å·²å­˜åœ¨è®°å½•çš„äº‹ä»¶ç±»å‹ï¼Œç¡®ä¿æ›´æ–°ç¡®å®å‘ç”Ÿï¼Œé¿å…â€œçœ‹ä¼¼æˆåŠŸä½†æ•°æ®æœªå†™å…¥â€
	if req.EventType == "update" && result != nil {
		if rows, err := result.RowsAffected(); err == nil && rows == 0 {
			return fmt.Errorf("no rows updated for event_type=update")
		}
	}

	err = tx.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}

	committed = true
	return nil
}

// executeWrite æ‰§è¡Œå•ä¸ªå†™æ“ä½œ
func (ut *UsageTracker) executeWrite(req WriteRequest) error {
	ut.writeMu.Lock()
	defer ut.writeMu.Unlock()

	ctx, cancel := context.WithTimeout(req.Context, 60*time.Second)
	defer cancel()

	// å®‰å…¨çš„äº‹åŠ¡å¤„ç†
	return ut.executeWriteTransaction(ctx, req)
}

// executeWriteTransaction æ‰§è¡Œå†™äº‹åŠ¡ï¼ˆä¿®å¤defer tx.Rollback()é—®é¢˜ï¼‰
func (ut *UsageTracker) executeWriteTransaction(ctx context.Context, req WriteRequest) error {
	tx, err := ut.writeDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}

	committed := false
	defer func() {
		if !committed {
			if rbErr := tx.Rollback(); rbErr != nil {
				slog.Error("Failed to rollback transaction", "error", rbErr, "event_type", req.EventType)
			}
		}
	}()

	_, err = tx.ExecContext(ctx, req.Query, req.Args...)
	if err != nil {
		return fmt.Errorf("failed to execute query: %w", err)
	}

	err = tx.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}

	committed = true // æ ‡è®°å·²æäº¤ï¼Œé¿å…é‡å¤Rollback
	return nil
}

// initDatabaseWithWriteDB ä½¿ç”¨å†™è¿æ¥åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåŒæ­¥ç­‰å¾…å®Œæˆï¼‰
func (ut *UsageTracker) initDatabaseWithWriteDB() error {
	// è¯»å–å¹¶æ‰§è¡Œ schema SQL
	schemaSQL, err := schemaFS.ReadFile("schema.sql")
	if err != nil {
		return fmt.Errorf("failed to read schema.sql: %w", err)
	}

	// ä½¿ç”¨å†™è¿æ¥ç›´æ¥æ‰§è¡Œ schemaï¼ˆåŒæ­¥æ–¹å¼ï¼Œç¡®ä¿è¡¨åˆ›å»ºå®Œæˆï¼‰
	if _, err := ut.writeDB.Exec(string(schemaSQL)); err != nil {
		return fmt.Errorf("failed to execute schema: %w", err)
	}

	slog.Debug("Database schema initialized successfully with write connection")
	return nil
}

// ============================================================================
// ğŸ”¥ v4.1 çƒ­æ± ç›¸å…³æ–¹æ³•
// ============================================================================

// GetActiveRequests è·å–çƒ­æ± ä¸­çš„æ´»è·ƒè¯·æ±‚
// è¿”å›å½“å‰æ­£åœ¨å¤„ç†ä¸­çš„è¯·æ±‚åˆ—è¡¨ï¼ˆå†…å­˜ä¸­ï¼‰
func (ut *UsageTracker) GetActiveRequests() []*ActiveRequest {
	if ut.hotPool == nil {
		return nil
	}
	return ut.hotPool.GetActive()
}

// GetActiveRequestCount è·å–æ´»è·ƒè¯·æ±‚æ•°é‡
func (ut *UsageTracker) GetActiveRequestCount() int {
	if ut.hotPool == nil {
		return 0
	}
	return ut.hotPool.GetActiveCount()
}

// GetHotPoolStats è·å–çƒ­æ± ç»Ÿè®¡ä¿¡æ¯
func (ut *UsageTracker) GetHotPoolStats() *HotPoolStats {
	if ut.hotPool == nil {
		return nil
	}
	stats := ut.hotPool.GetStats()
	return &stats
}

// GetArchiveStats è·å–å½’æ¡£ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
func (ut *UsageTracker) GetArchiveStats() *ArchiveStats {
	if ut.archiveManager == nil {
		return nil
	}
	stats := ut.archiveManager.GetStats()
	return &stats
}

// IsHotPoolEnabled æ£€æŸ¥çƒ­æ± æ¨¡å¼æ˜¯å¦å¯ç”¨
func (ut *UsageTracker) IsHotPoolEnabled() bool {
	return ut.hotPoolEnabled
}

// GetActiveRequest è·å–å•ä¸ªæ´»è·ƒè¯·æ±‚
func (ut *UsageTracker) GetActiveRequest(requestID string) (*ActiveRequest, bool) {
	if ut.hotPool == nil {
		return nil, false
	}
	return ut.hotPool.Get(requestID)
}

// IsRequestActive æ£€æŸ¥è¯·æ±‚æ˜¯å¦åœ¨çƒ­æ± ä¸­
func (ut *UsageTracker) IsRequestActive(requestID string) bool {
	if ut.hotPool == nil {
		return false
	}
	return ut.hotPool.Exists(requestID)
}

// UpdateActiveRequestTokens æ›´æ–°æ´»è·ƒè¯·æ±‚çš„Tokenï¼ˆæµå¼è¯·æ±‚ç´¯ç§¯ï¼‰
func (ut *UsageTracker) UpdateActiveRequestTokens(requestID string, inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64) error {
	if ut.hotPool == nil {
		return fmt.Errorf("hot pool not enabled")
	}

	return ut.hotPool.Update(requestID, func(req *ActiveRequest) {
		req.InputTokens = inputTokens
		req.OutputTokens = outputTokens
		req.CacheCreationTokens = cacheCreationTokens
		req.CacheReadTokens = cacheReadTokens
	})
}

// GetHotPoolAndArchiveOverview è·å–çƒ­æ± å’Œå½’æ¡£æ¦‚è§ˆï¼ˆç”¨äºç›‘æ§ï¼‰
func (ut *UsageTracker) GetHotPoolAndArchiveOverview() map[string]interface{} {
	overview := map[string]interface{}{
		"hot_pool_enabled": ut.hotPoolEnabled,
	}

	if ut.hotPool != nil {
		stats := ut.hotPool.GetStats()
		overview["hot_pool"] = map[string]interface{}{
			"current_size":   stats.CurrentSize,
			"peak_size":      stats.PeakSize,
			"total_added":    stats.TotalAdded,
			"total_removed":  stats.TotalRemoved,
			"total_archived": stats.TotalArchived,
			"total_expired":  stats.TotalExpired,
			"total_overflow": stats.TotalOverflow,
		}
	}

	if ut.archiveManager != nil {
		stats := ut.archiveManager.GetStats()
		overview["archive_manager"] = map[string]interface{}{
			"total_received":     stats.TotalReceived,
			"total_archived":     stats.TotalArchived,
			"total_failed":       stats.TotalFailed,
			"total_dropped":      stats.TotalDropped,
			"total_batches":      stats.TotalBatches,
			"avg_batch_size":     stats.AvgBatchSize,
			"channel_length":     stats.ChannelLength,
			"last_flush_time":    stats.LastFlushTime,
			"last_flush_latency": stats.LastFlushLatency.String(),
		}
	}

	return overview
}

// ============================================================================
// ğŸ”¥ v4.1 åŒæºæŸ¥è¯¢æ–¹æ³•ï¼ˆçƒ­æ±  + æ•°æ®åº“ï¼‰
// ============================================================================

// ActiveRequestToDetail å°† ActiveRequest è½¬æ¢ä¸º RequestDetail
func (ut *UsageTracker) ActiveRequestToDetail(req *ActiveRequest) RequestDetail {
	var endTime *time.Time
	var durationMs *int64
	var httpStatus *int

	if req.EndTime != nil {
		endTime = req.EndTime
	}
	if req.DurationMs > 0 {
		durationMs = &req.DurationMs
	}
	if req.HTTPStatus > 0 {
		httpStatus = &req.HTTPStatus
	}

	// è®¡ç®—æˆæœ¬ï¼ˆä½¿ç”¨å…¬å…±å‡½æ•°æ¶ˆé™¤é‡å¤ä»£ç ï¼‰
	var cost CostBreakdown
	if ut.pricing != nil {
		pricing, exists := ut.pricing[req.ModelName]
		if !exists {
			pricing, exists = ut.pricing["_default"]
		}
		if exists {
			// è·å–ç«¯ç‚¹å€ç‡
			var multiplier *EndpointMultiplier
			if ut.endpointMu != nil {
				if m, ok := ut.endpointMu[req.EndpointName]; ok {
					multiplier = &m
				}
			}

			// è°ƒç”¨å…¬å…±æˆæœ¬è®¡ç®—å‡½æ•°
			// TODO: éœ€è¦ä»è¯·æ±‚ä¸­è¯†åˆ«ç¼“å­˜ç±»å‹ï¼ˆ5åˆ†é’Ÿ vs 1å°æ—¶ï¼‰ï¼Œæš‚æ—¶é»˜è®¤ä½¿ç”¨ 5 åˆ†é’Ÿå€ç‡
			cost = CalculateCost(
				req.InputTokens, req.OutputTokens, req.CacheCreationTokens, req.CacheReadTokens,
				&pricing, multiplier, false,
			)
		}
	}

	return RequestDetail{
		ID:                    0, // çƒ­æ± ä¸­çš„è¯·æ±‚è¿˜æ²¡æœ‰æ•°æ®åº“ID
		RequestID:             req.RequestID,
		ClientIP:              req.ClientIP,
		UserAgent:             req.UserAgent,
		Method:                req.Method,
		Path:                  req.Path,
		StartTime:             req.StartTime,
		EndTime:               endTime,
		DurationMs:            durationMs,
		EndpointName:          req.EndpointName,
		Channel:               req.Channel, // v5.0: æ¸ é“æ ‡ç­¾
		GroupName:             req.GroupName,
		ModelName:             req.ModelName,
		AuthType:              req.AuthType,
		AuthKey:               req.AuthKey,
		IsStreaming:           req.IsStreaming,
		Status:                req.Status,
		HTTPStatusCode:        httpStatus,
		RetryCount:            req.RetryCount,
		FailureReason:         req.FailureReason,
		LastFailureReason:     "",
		CancelReason:          req.CancelReason,
		InputTokens:           req.InputTokens,
		OutputTokens:          req.OutputTokens,
		CacheCreationTokens:   req.CacheCreationTokens,
		CacheCreation5mTokens: req.CacheCreation5mTokens, // v5.0.1+
		CacheCreation1hTokens: req.CacheCreation1hTokens, // v5.0.1+
		CacheReadTokens:       req.CacheReadTokens,
		InputCostUSD:          cost.InputCost,
		OutputCostUSD:         cost.OutputCost,
		CacheCreationCostUSD:  cost.CacheCreationCost,
		CacheReadCostUSD:      cost.CacheReadCost,
		TotalCostUSD:          cost.TotalCost,
		CreatedAt:             req.StartTime,
		UpdatedAt:             ut.now(),
	}
}

// QueryRequestDetailsWithHotPool åŒæºæŸ¥è¯¢ï¼šçƒ­æ±  + æ•°æ®åº“
// è¿”å›åˆå¹¶åçš„è¯·æ±‚åˆ—è¡¨ï¼Œçƒ­æ± ä¸­çš„æ´»è·ƒè¯·æ±‚æ’åœ¨å‰é¢
// ğŸ”§ [ä¿®å¤] 2025-12-11: æ­£ç¡®å¤„ç†åˆ†é¡µï¼Œç¡®ä¿è¿”å›æ•°é‡ä¸è¶…è¿‡ limit
func (ut *UsageTracker) QueryRequestDetailsWithHotPool(ctx context.Context, opts *QueryOptions) ([]RequestDetail, int64, error) {
	var results []RequestDetail

	// è·å–åˆ†é¡µå‚æ•°
	limit := opts.Limit
	offset := opts.Offset
	if limit <= 0 {
		limit = 20 // é»˜è®¤æ¯é¡µ20æ¡
	}

	// 1. ä»çƒ­æ± è·å–æ‰€æœ‰ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„æ´»è·ƒè¯·æ±‚ï¼ˆç”¨äºè®¡ç®—æ€»æ•°å’Œåˆ†é¡µï¼‰
	allHotPoolRequests := ut.getFilteredHotPoolRequests(opts)
	hotPoolCount := len(allHotPoolRequests)

	// 2. è·å–æ•°æ®åº“æ€»æ•°ï¼ˆç”¨äºåˆ†é¡µè®¡ç®—ï¼‰
	dbCount, err := ut.CountRequestDetails(ctx, opts)
	if err != nil {
		slog.Warn("Failed to count database requests", "error", err)
		dbCount = 0
	}

	// 3. è®¡ç®—æ€»æ•°ï¼šçƒ­æ±  + æ•°æ®åº“
	totalCount := int64(hotPoolCount) + int64(dbCount)

	// 4. çƒ­æ± æ•°æ®æŒ‰å¼€å§‹æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
	if hotPoolCount > 0 {
		sort.Slice(allHotPoolRequests, func(i, j int) bool {
			return allHotPoolRequests[i].StartTime.After(allHotPoolRequests[j].StartTime)
		})
	}

	// åˆ›å»ºçƒ­æ± è¯·æ±‚IDé›†åˆç”¨äºå»é‡
	hotPoolIDs := make(map[string]bool)
	for _, req := range allHotPoolRequests {
		hotPoolIDs[req.RequestID] = true
	}

	// 5. æ ¹æ® offset å’Œ limit å†³å®šä»å“ªé‡Œå–æ•°æ®
	if offset < hotPoolCount {
		// å½“å‰é¡µåŒ…å«çƒ­æ± æ•°æ®
		hotPoolEnd := offset + limit
		if hotPoolEnd > hotPoolCount {
			hotPoolEnd = hotPoolCount
		}
		// ä»çƒ­æ± å–æ•°æ®
		results = append(results, allHotPoolRequests[offset:hotPoolEnd]...)

		// å¦‚æœçƒ­æ± æ•°æ®ä¸å¤Ÿä¸€é¡µï¼Œä»æ•°æ®åº“è¡¥å……
		remaining := limit - len(results)
		if remaining > 0 {
			// ä»æ•°æ®åº“æŸ¥è¯¢ï¼Œoffset=0 å› ä¸ºçƒ­æ± æ•°æ®å·²ç»å æ®äº†å‰é¢çš„ä½ç½®
			dbOpts := *opts
			dbOpts.Offset = 0
			dbOpts.Limit = remaining + hotPoolCount // å¤šå–ä¸€äº›ç”¨äºå»é‡
			dbRequests, err := ut.QueryRequestDetails(ctx, &dbOpts)
			if err != nil {
				slog.Warn("Failed to query database requests", "error", err)
			} else {
				// æ·»åŠ æ•°æ®åº“è¯·æ±‚ï¼ˆæ’é™¤å·²åœ¨çƒ­æ± ä¸­çš„ï¼‰
				for _, req := range dbRequests {
					if !hotPoolIDs[req.RequestID] && len(results) < limit {
						results = append(results, req)
					}
				}
			}
		}
	} else {
		// å½“å‰é¡µåªæœ‰æ•°æ®åº“æ•°æ®
		// è°ƒæ•´æ•°æ®åº“æŸ¥è¯¢çš„ offsetï¼ˆå‡å»çƒ­æ± æ•°æ®çš„æ•°é‡ï¼‰
		dbOpts := *opts
		dbOpts.Offset = offset - hotPoolCount
		dbOpts.Limit = limit + hotPoolCount // å¤šå–ä¸€äº›ç”¨äºå»é‡

		dbRequests, err := ut.QueryRequestDetails(ctx, &dbOpts)
		if err != nil {
			return nil, 0, fmt.Errorf("failed to query database: %w", err)
		}

		// æ·»åŠ æ•°æ®åº“è¯·æ±‚ï¼ˆæ’é™¤å·²åœ¨çƒ­æ± ä¸­çš„ï¼‰
		for _, req := range dbRequests {
			if !hotPoolIDs[req.RequestID] && len(results) < limit {
				results = append(results, req)
			}
		}
	}

	return results, totalCount, nil
}

// getFilteredHotPoolRequests è·å–è¿‡æ»¤åçš„çƒ­æ± è¯·æ±‚
func (ut *UsageTracker) getFilteredHotPoolRequests(opts *QueryOptions) []RequestDetail {
	if ut.hotPool == nil {
		return nil
	}

	activeRequests := ut.hotPool.GetActive()
	if len(activeRequests) == 0 {
		return nil
	}

	var filtered []RequestDetail
	for _, req := range activeRequests {
		// åº”ç”¨è¿‡æ»¤æ¡ä»¶
		if opts != nil {
			// çŠ¶æ€è¿‡æ»¤
			if opts.Status != "" && req.Status != opts.Status {
				continue
			}
			// æ¨¡å‹è¿‡æ»¤
			if opts.ModelName != "" && req.ModelName != opts.ModelName {
				continue
			}
			// æ¸ é“è¿‡æ»¤ï¼ˆv5.0ï¼‰
			if opts.Channel != "" && req.Channel != opts.Channel {
				continue
			}
			// ç«¯ç‚¹è¿‡æ»¤
			if opts.EndpointName != "" && req.EndpointName != opts.EndpointName {
				continue
			}
			// ç»„è¿‡æ»¤
			if opts.GroupName != "" && req.GroupName != opts.GroupName {
				continue
			}
			// æ—¶é—´èŒƒå›´è¿‡æ»¤
			if opts.StartDate != nil && req.StartTime.Before(*opts.StartDate) {
				continue
			}
			if opts.EndDate != nil && req.StartTime.After(*opts.EndDate) {
				continue
			}
		}

		filtered = append(filtered, ut.ActiveRequestToDetail(req))
	}

	return filtered
}

// GetHotPoolRequestCount è·å–çƒ­æ± ä¸­ç¬¦åˆæ¡ä»¶çš„è¯·æ±‚æ•°é‡
func (ut *UsageTracker) GetHotPoolRequestCount(opts *QueryOptions) int {
	filtered := ut.getFilteredHotPoolRequests(opts)
	return len(filtered)
}
