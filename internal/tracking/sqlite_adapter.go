package tracking

import (
	"context"
	"database/sql"
	"embed"
	"fmt"
	"log/slog"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"time"

	_ "modernc.org/sqlite"
)

//go:embed schema.sql
var sqliteSchemaFS embed.FS

// SQLiteAdapter SQLiteæ•°æ®åº“é€‚é…å™¨å®ç°ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
type SQLiteAdapter struct {
	config   DatabaseConfig
	db       *sql.DB
	logger   *slog.Logger
	location *time.Location // é…ç½®çš„æ—¶åŒº
}

// NewSQLiteAdapter åˆ›å»ºSQLiteé€‚é…å™¨å®ä¾‹
func NewSQLiteAdapter(config DatabaseConfig) (*SQLiteAdapter, error) {
	// è®¾ç½®é»˜è®¤é…ç½®
	setDefaultConfig(&config)

	// è§£ææ—¶åŒºé…ç½®
	timezone := strings.TrimSpace(config.Timezone)
	if timezone == "" {
		timezone = "Asia/Shanghai" // é»˜è®¤æ—¶åŒº
	}

	location, err := time.LoadLocation(timezone)
	if err != nil {
		// å¦‚æœæ—¶åŒºè§£æå¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸ç»ˆæ­¢ï¼Œä½¿ç”¨ç³»ç»Ÿæœ¬åœ°æ—¶åŒº
		location = time.Local
		slog.Warn("SQLiteæ—¶åŒºè§£æå¤±è´¥ï¼Œä½¿ç”¨ç³»ç»Ÿæœ¬åœ°æ—¶åŒº",
			"configured_timezone", timezone,
			"error", err,
			"fallback_timezone", location.String())
	} else {
		slog.Info("SQLiteæ—¶åŒºé…ç½®æˆåŠŸ", "timezone", timezone)
	}

	adapter := &SQLiteAdapter{
		config:   config,
		logger:   slog.Default(),
		location: location,
	}

	return adapter, nil
}

// Open å»ºç«‹SQLiteæ•°æ®åº“è¿æ¥
func (s *SQLiteAdapter) Open() error {
	dbPath := s.config.DatabasePath
	if dbPath == "" {
		// ä½¿ç”¨è·¨å¹³å°ç”¨æˆ·ç›®å½•ä½œä¸ºé»˜è®¤è·¯å¾„
		// Windows: %APPDATA%\CC-Forwarder\data\cc-forwarder.db
		// macOS: ~/Library/Application Support/CC-Forwarder/data/cc-forwarder.db
		// Linux: ~/.local/share/cc-forwarder/data/cc-forwarder.db
		dbPath = filepath.Join(getSQLiteAppDataDir(), "data", "cc-forwarder.db")
		s.logger.Info("ä½¿ç”¨é»˜è®¤æ•°æ®åº“è·¯å¾„", "path", dbPath)
	}

	s.logger.Info("æ­£åœ¨è¿æ¥SQLiteæ•°æ®åº“", "path", dbPath)

	// ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
	if dbPath != ":memory:" {
		dbDir := filepath.Dir(dbPath)
		if err := os.MkdirAll(dbDir, 0755); err != nil {
			return fmt.Errorf("failed to create database directory: %w", err)
		}
	}

	// æ„å»ºSQLiteè¿æ¥å­—ç¬¦ä¸²
	dsn := dbPath + "?_journal_mode=WAL&_synchronous=NORMAL&_cache_size=10000&_foreign_keys=1&_busy_timeout=60000"

	db, err := sql.Open("sqlite", dsn)
	if err != nil {
		return fmt.Errorf("failed to open SQLite database: %w", err)
	}

	// è®¾ç½®è¿æ¥æ± å‚æ•°ï¼ˆSQLiteå»ºè®®å°‘é‡è¿æ¥ï¼‰
	db.SetMaxOpenConns(1) // SQLiteå†™æ“ä½œéœ€è¦å•ä¸€è¿æ¥
	db.SetMaxIdleConns(1)
	db.SetConnMaxLifetime(0)

	// æµ‹è¯•è¿æ¥
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := db.PingContext(ctx); err != nil {
		db.Close()
		return fmt.Errorf("failed to ping SQLite database: %w", err)
	}

	s.db = db

	// è¯Šæ–­æ—¶åŒºè®¾ç½®
	s.diagnoseTimezoneSettings()

	s.logger.Info("âœ… SQLiteæ•°æ®åº“è¿æ¥æˆåŠŸ")

	return nil
}

// Close å…³é—­æ•°æ®åº“è¿æ¥
func (s *SQLiteAdapter) Close() error {
	if s.db != nil {
		s.logger.Info("æ­£åœ¨å…³é—­SQLiteæ•°æ®åº“è¿æ¥")
		return s.db.Close()
	}
	return nil
}

// Ping æµ‹è¯•æ•°æ®åº“è¿æ¥
func (s *SQLiteAdapter) Ping(ctx context.Context) error {
	if s.db == nil {
		return fmt.Errorf("database not connected")
	}
	return s.db.PingContext(ctx)
}

// GetDB è·å–æ•°æ®åº“è¿æ¥
func (s *SQLiteAdapter) GetDB() *sql.DB {
	return s.db
}

// GetReadDB è·å–è¯»æ•°æ®åº“è¿æ¥
func (s *SQLiteAdapter) GetReadDB() *sql.DB {
	return s.db
}

// GetWriteDB è·å–å†™æ•°æ®åº“è¿æ¥
func (s *SQLiteAdapter) GetWriteDB() *sql.DB {
	return s.db
}

// BeginTx å¼€å§‹äº‹åŠ¡
func (s *SQLiteAdapter) BeginTx(ctx context.Context, opts *sql.TxOptions) (*sql.Tx, error) {
	if s.db == nil {
		return nil, fmt.Errorf("database not connected")
	}
	return s.db.BeginTx(ctx, opts)
}

// InitSchema åˆå§‹åŒ–SQLiteæ•°æ®åº“Schema
func (s *SQLiteAdapter) InitSchema() error {
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	s.logger.Info("æ­£åœ¨åˆå§‹åŒ–SQLiteæ•°æ®åº“Schema")

	// è¯»å–å¹¶æ‰§è¡ŒSQLite schema
	schema, err := sqliteSchemaFS.ReadFile("schema.sql")
	if err != nil {
		return fmt.Errorf("failed to read schema.sql: %w", err)
	}

	// SQLiteå¯ä»¥ç›´æ¥æ‰§è¡Œæ•´ä¸ªschemaã€‚
	// ä½†ï¼šæ—§åº“å¯èƒ½ç¼ºå°‘åç»­æ–°å¢çš„åˆ—ï¼Œschema.sql ä¸­çš„ç´¢å¼•/è§¦å‘å™¨å¯èƒ½ä¼šå¼•ç”¨è¿™äº›åˆ—ï¼Œ
	// å¯¼è‡´ Exec ç›´æ¥å¤±è´¥å¹¶ä¸­æ–­å¯åŠ¨ã€‚ä¸ºä¿è¯å‘åå…¼å®¹ï¼š
	// - å…ˆå°è¯•æ‰§è¡Œ schema.sql
	// - è‹¥å› â€œno such columnâ€å¤±è´¥ï¼Œåˆ™å…ˆè·‘ migrateSchema è¡¥åˆ—ï¼Œå†é‡è¯•æ‰§è¡Œ schema.sql
	if _, err := s.db.ExecContext(ctx, string(schema)); err != nil {
		if strings.Contains(err.Error(), "no such column:") {
			s.logger.Warn("schema.sql æ‰§è¡Œå¤±è´¥ï¼ˆç¼ºå°‘åˆ—ï¼‰ï¼Œå°†å…ˆæ‰§è¡Œè¿ç§»åé‡è¯•", "error", err)
			if err := s.migrateSchema(ctx); err != nil {
				return fmt.Errorf("failed to migrate schema (pre-schema retry): %w", err)
			}
			if _, err := s.db.ExecContext(ctx, string(schema)); err != nil {
				return fmt.Errorf("failed to execute schema (after migrate): %w", err)
			}
		} else {
			return fmt.Errorf("failed to execute schema: %w", err)
		}
	}

	// v5.0.1+: æ‰§è¡Œè¿ç§»æ·»åŠ æ–°å­—æ®µ
	if err := s.migrateSchema(ctx); err != nil {
		return fmt.Errorf("failed to migrate schema: %w", err)
	}

	s.logger.Info("âœ… SQLiteæ•°æ®åº“Schemaåˆå§‹åŒ–å®Œæˆ")
	return nil
}

// migrateSchema æ‰§è¡Œæ•°æ®åº“è¿ç§»ï¼ˆv5.0.1+: æ·»åŠ  5m/1h ç¼“å­˜å­—æ®µï¼‰
func (s *SQLiteAdapter) migrateSchema(ctx context.Context) error {
	// request_logs è¿ç§»ï¼šå†å²ä¸Šå…ˆä¸Šçº¿ usage trackingï¼Œåç»­è¡¥å……ç¼“å­˜å­—æ®µ
	requestLogMigrations := []struct {
		checkColumn string
		alterSQL    string
		description string
	}{
		{
			checkColumn: "cache_creation_5m_tokens",
			alterSQL:    "ALTER TABLE request_logs ADD COLUMN cache_creation_5m_tokens INTEGER DEFAULT 0",
			description: "5åˆ†é’Ÿç¼“å­˜åˆ›å»ºtokenså­—æ®µ",
		},
		{
			checkColumn: "cache_creation_1h_tokens",
			alterSQL:    "ALTER TABLE request_logs ADD COLUMN cache_creation_1h_tokens INTEGER DEFAULT 0",
			description: "1å°æ—¶ç¼“å­˜åˆ›å»ºtokenså­—æ®µ",
		},
		{
			checkColumn: "cache_creation_5m_cost_usd",
			alterSQL:    "ALTER TABLE request_logs ADD COLUMN cache_creation_5m_cost_usd REAL DEFAULT 0",
			description: "5åˆ†é’Ÿç¼“å­˜åˆ›å»ºæˆæœ¬å­—æ®µ",
		},
		{
			checkColumn: "cache_creation_1h_cost_usd",
			alterSQL:    "ALTER TABLE request_logs ADD COLUMN cache_creation_1h_cost_usd REAL DEFAULT 0",
			description: "1å°æ—¶ç¼“å­˜åˆ›å»ºæˆæœ¬å­—æ®µ",
		},
	}

	// endpoints è¿ç§»ï¼šç«¯ç‚¹å­˜å‚¨è¡¨è¿­ä»£æ–°å¢å­—æ®µæ—¶ï¼Œéœ€è¦å…¼å®¹æ—§ dbï¼ˆCREATE TABLE IF NOT EXISTS ä¸ä¼šè¡¥åˆ—ï¼‰
	endpointsMigrations := []struct {
		checkColumn string
		alterSQL    string
		description string
	}{
		{
			checkColumn: "timeout_seconds",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN timeout_seconds INTEGER DEFAULT 300",
			description: "ç«¯ç‚¹è¶…æ—¶å­—æ®µ",
		},
		{
			checkColumn: "supports_count_tokens",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN supports_count_tokens INTEGER DEFAULT 0",
			description: "ç«¯ç‚¹ supports_count_tokens å­—æ®µ",
		},
		{
			checkColumn: "cost_multiplier",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN cost_multiplier REAL DEFAULT 1.0",
			description: "ç«¯ç‚¹æ€»æˆæœ¬å€ç‡å­—æ®µ",
		},
		{
			checkColumn: "input_cost_multiplier",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN input_cost_multiplier REAL DEFAULT 1.0",
			description: "ç«¯ç‚¹è¾“å…¥æˆæœ¬å€ç‡å­—æ®µ",
		},
		{
			checkColumn: "output_cost_multiplier",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN output_cost_multiplier REAL DEFAULT 1.0",
			description: "ç«¯ç‚¹è¾“å‡ºæˆæœ¬å€ç‡å­—æ®µ",
		},
		{
			checkColumn: "cache_creation_cost_multiplier",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN cache_creation_cost_multiplier REAL DEFAULT 1.0",
			description: "ç«¯ç‚¹ 5m ç¼“å­˜åˆ›å»ºæˆæœ¬å€ç‡å­—æ®µ",
		},
		{
			checkColumn: "cache_creation_cost_multiplier_1h",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN cache_creation_cost_multiplier_1h REAL DEFAULT 1.0",
			description: "ç«¯ç‚¹ 1h ç¼“å­˜åˆ›å»ºæˆæœ¬å€ç‡å­—æ®µ",
		},
		{
			checkColumn: "cache_read_cost_multiplier",
			alterSQL:    "ALTER TABLE endpoints ADD COLUMN cache_read_cost_multiplier REAL DEFAULT 1.0",
			description: "ç«¯ç‚¹ç¼“å­˜è¯»å–æˆæœ¬å€ç‡å­—æ®µ",
		},
	}

	// channels è¿ç§»ï¼šæ—©æœŸå¯èƒ½åªæœ‰ nameï¼Œåç»­æ–°å¢ website
	channelMigrations := []struct {
		checkColumn string
		alterSQL    string
		description string
	}{
		{
			checkColumn: "website",
			alterSQL:    "ALTER TABLE channels ADD COLUMN website TEXT",
			description: "æ¸ é“å®˜ç½‘å­—æ®µ",
		},
		{
			checkColumn: "priority",
			alterSQL:    "ALTER TABLE channels ADD COLUMN priority INTEGER DEFAULT 1",
			description: "æ¸ é“ä¼˜å…ˆçº§å­—æ®µ",
		},
		{
			checkColumn: "failover_enabled",
			alterSQL:    "ALTER TABLE channels ADD COLUMN failover_enabled INTEGER DEFAULT 1",
			description: "æ¸ é“æ•…éšœè½¬ç§»å¼€å…³å­—æ®µ",
		},
	}

	runMigrations := func(table string, migrations []struct {
		checkColumn string
		alterSQL    string
		description string
	}) error {
		tableExists, err := s.tableExists(ctx, table)
		if err != nil {
			return fmt.Errorf("failed to check table %s: %w", table, err)
		}
		if !tableExists {
			return nil
		}

		for _, m := range migrations {
			exists, err := s.columnExists(ctx, table, m.checkColumn)
			if err != nil {
				return fmt.Errorf("failed to check column %s.%s: %w", table, m.checkColumn, err)
			}
			if exists {
				continue
			}

			s.logger.Info(fmt.Sprintf("ğŸ”§ [æ•°æ®åº“è¿ç§»] %sï¼šæ·»åŠ  %s", table, m.description))
			if _, err := s.db.ExecContext(ctx, m.alterSQL); err != nil {
				return fmt.Errorf("failed to add column %s.%s: %w", table, m.checkColumn, err)
			}
			s.logger.Info(fmt.Sprintf("âœ… [æ•°æ®åº“è¿ç§»] %sï¼š%s æ·»åŠ æˆåŠŸ", table, m.description))
		}
		return nil
	}

	if err := runMigrations("request_logs", requestLogMigrations); err != nil {
		return err
	}
	if err := runMigrations("endpoints", endpointsMigrations); err != nil {
		return err
	}
	// v6.2+: å…è®¸ä¸åŒæ¸ é“ç«¯ç‚¹åŒåï¼ˆçº¦æŸä» name å…¨å±€å”¯ä¸€è°ƒæ•´ä¸º (channel,name) æ¸ é“å†…å”¯ä¸€ï¼‰ã€‚
	// SQLite æ— æ³•ç›´æ¥ç§»é™¤æ—§ UNIQUE(name) çº¦æŸï¼Œéœ€è¦åœ¨å‘ç°æ—§çº¦æŸæ—¶é‡å»ºè¡¨ã€‚
	if err := s.ensureEndpointsUniqueByChannelAndName(ctx); err != nil {
		return err
	}
	if err := runMigrations("channels", channelMigrations); err != nil {
		return err
	}

	return nil
}

func (s *SQLiteAdapter) ensureEndpointsUniqueByChannelAndName(ctx context.Context) error {
	tableExists, err := s.tableExists(ctx, "endpoints")
	if err != nil {
		return fmt.Errorf("failed to check table endpoints: %w", err)
	}
	if !tableExists {
		return nil
	}

	hasLegacyUniqueOnName, hasDesiredUnique, err := s.detectEndpointsUniqueIndexes(ctx)
	if err != nil {
		return err
	}
	if hasDesiredUnique && !hasLegacyUniqueOnName {
		return nil
	}

	// è‹¥å­˜åœ¨æ—§ UNIQUE(name)ï¼Œå¿…é¡»é‡å»ºè¡¨ï¼Œå¦åˆ™å³ä½¿è¡¥ä¸€ä¸ªæ–°ç´¢å¼•ä¹Ÿä»ä¼šè¢«æ—§çº¦æŸæ‹¦ä½ã€‚
	if hasLegacyUniqueOnName {
		s.logger.Info("ğŸ”§ [æ•°æ®åº“è¿ç§»] endpointsï¼šæ£€æµ‹åˆ°æ—§ UNIQUE(name)ï¼Œå°†é‡å»ºä¸º UNIQUE(channel,name)")
		return s.rebuildEndpointsTableForChannelScopedUniq(ctx)
	}

	// æ²¡æœ‰æ—§çº¦æŸä½†ä¹Ÿæ²¡æœ‰æ–°çº¦æŸï¼šè¡¥ä¸€ä¸ªå”¯ä¸€ç´¢å¼•å³å¯ã€‚
	_, err = s.db.ExecContext(ctx, "CREATE UNIQUE INDEX IF NOT EXISTS idx_endpoints_channel_name_unique ON endpoints(channel, name)")
	if err != nil {
		return fmt.Errorf("failed to create unique index endpoints(channel,name): %w", err)
	}
	return nil
}

func (s *SQLiteAdapter) detectEndpointsUniqueIndexes(ctx context.Context) (hasLegacyUniqueOnName bool, hasDesiredUnique bool, _ error) {
	rows, err := s.db.QueryContext(ctx, "PRAGMA index_list(endpoints)")
	if err != nil {
		return false, false, fmt.Errorf("failed to query endpoints indexes: %w", err)
	}
	// æ³¨æ„ï¼šdatabase/sql åœ¨ *sql.Rows æœª Close å‰ä¼šå ç”¨è¿æ¥ã€‚
	// SQLite è¿æ¥æ± é€šå¸¸é™åˆ¶ä¸º 1ï¼ˆè§ Open: SetMaxOpenConns(1)ï¼‰ï¼Œå¦‚æœåœ¨éå† rows æ—¶å†å‘èµ·åµŒå¥— Queryï¼Œ
	// ä¼šå› æ‹¿ä¸åˆ°æ–°è¿æ¥è€Œé˜»å¡ï¼Œæœ€ç»ˆè§¦å‘ ctx è¶…æ—¶ï¼ˆè¡¨ç°ä¸º context deadline exceededï¼‰ã€‚
	// å› æ­¤è¿™é‡Œå…ˆæŠŠéœ€è¦çš„ç´¢å¼•åè¯»å‡ºæ¥ï¼Œå†é€ä¸ªæŸ¥è¯¢ index_infoã€‚

	type idx struct {
		name string
	}
	uniqueIndexes := make([]idx, 0, 4)
	for rows.Next() {
		var (
			seq     int
			name    string
			unique  int
			origin  string
			partial int
		)
		if err := rows.Scan(&seq, &name, &unique, &origin, &partial); err != nil {
			rows.Close()
			return false, false, fmt.Errorf("failed to scan endpoints index: %w", err)
		}
		if unique != 1 {
			continue
		}
		uniqueIndexes = append(uniqueIndexes, idx{name: name})
	}
	if err := rows.Err(); err != nil {
		rows.Close()
		return false, false, fmt.Errorf("failed to iterate endpoints index_list: %w", err)
	}
	rows.Close()

	for _, it := range uniqueIndexes {
		// index åæ¥è‡ª sqlite_master/pragma è¾“å‡ºï¼Œä½†ä»åšç®€å•è½¬ä¹‰é¿å…æ‹¼æ¥é—®é¢˜
		escaped := strings.ReplaceAll(it.name, "'", "''")
		colRows, err := s.db.QueryContext(ctx, "PRAGMA index_info('"+escaped+"')")
		if err != nil {
			return false, false, fmt.Errorf("failed to query endpoints index_info(%s): %w", it.name, err)
		}

		cols := make([]string, 0, 2)
		for colRows.Next() {
			var seqno, cid int
			var colName string
			if err := colRows.Scan(&seqno, &cid, &colName); err != nil {
				colRows.Close()
				return false, false, fmt.Errorf("failed to scan endpoints index_info(%s): %w", it.name, err)
			}
			cols = append(cols, colName)
		}
		colRows.Close()

		if len(cols) == 1 && cols[0] == "name" {
			hasLegacyUniqueOnName = true
		}
		if len(cols) == 2 && cols[0] == "channel" && cols[1] == "name" {
			hasDesiredUnique = true
		}
	}

	return hasLegacyUniqueOnName, hasDesiredUnique, nil
}

func (s *SQLiteAdapter) rebuildEndpointsTableForChannelScopedUniq(ctx context.Context) error {
	// æ³¨æ„ï¼šè¯¥è¿ç§»ä¼šé”è¡¨å¹¶é‡å»º endpointsï¼Œä½†è¡¨ä½“é‡é€šå¸¸è¾ƒå°ï¼ˆé…ç½®è¡¨ï¼‰ï¼Œå¯æ¥å—ã€‚
	tx, err := s.db.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin endpoints rebuild tx: %w", err)
	}
	defer func() { _ = tx.Rollback() }()

	// æ¸…ç†æ—§è§¦å‘å™¨ï¼ˆè¡¨é‡å»ºå‰å…ˆåˆ ï¼Œé¿å…åå­—å†²çªï¼‰
	_, _ = tx.ExecContext(ctx, "DROP TRIGGER IF EXISTS update_endpoints_timestamp")

	// é‡å‘½åæ—§è¡¨
	if _, err := tx.ExecContext(ctx, "ALTER TABLE endpoints RENAME TO endpoints_old"); err != nil {
		return fmt.Errorf("failed to rename endpoints to endpoints_old: %w", err)
	}

	// åˆ›å»ºæ–°è¡¨ï¼ˆä¸ schema.sql ä¿æŒä¸€è‡´ï¼‰
	createSQL := `
CREATE TABLE endpoints (
    id INTEGER PRIMARY KEY AUTOINCREMENT,

    channel TEXT NOT NULL,
    name TEXT NOT NULL,
    url TEXT NOT NULL,

    token TEXT,
    api_key TEXT,
    headers TEXT,

    priority INTEGER DEFAULT 1,
    failover_enabled INTEGER DEFAULT 1,
    cooldown_seconds INTEGER,
    timeout_seconds INTEGER DEFAULT 300,

    supports_count_tokens INTEGER DEFAULT 0,

    cost_multiplier REAL DEFAULT 1.0,
    input_cost_multiplier REAL DEFAULT 1.0,
    output_cost_multiplier REAL DEFAULT 1.0,
    cache_creation_cost_multiplier REAL DEFAULT 1.0,
    cache_creation_cost_multiplier_1h REAL DEFAULT 1.0,
    cache_read_cost_multiplier REAL DEFAULT 1.0,

    enabled INTEGER DEFAULT 1,

    created_at DATETIME DEFAULT (strftime('%Y-%m-%d %H:%M:%f', 'now', 'localtime') || '+08:00'),
    updated_at DATETIME DEFAULT (strftime('%Y-%m-%d %H:%M:%f', 'now', 'localtime') || '+08:00'),

    UNIQUE(channel, name)
);`
	if _, err := tx.ExecContext(ctx, createSQL); err != nil {
		return fmt.Errorf("failed to create new endpoints table: %w", err)
	}

	// å¤åˆ¶æ•°æ®ï¼ˆä¿ç•™åŸ idï¼‰
	copySQL := `
INSERT INTO endpoints (
    id, channel, name, url, token, api_key, headers,
    priority, failover_enabled, cooldown_seconds, timeout_seconds,
    supports_count_tokens,
    cost_multiplier, input_cost_multiplier, output_cost_multiplier,
    cache_creation_cost_multiplier, cache_creation_cost_multiplier_1h, cache_read_cost_multiplier,
    enabled, created_at, updated_at
)
SELECT
    id, channel, name, url, token, api_key, headers,
    priority, failover_enabled, cooldown_seconds, timeout_seconds,
    supports_count_tokens,
    cost_multiplier, input_cost_multiplier, output_cost_multiplier,
    cache_creation_cost_multiplier, cache_creation_cost_multiplier_1h, cache_read_cost_multiplier,
    enabled, created_at, updated_at
FROM endpoints_old;`
	if _, err := tx.ExecContext(ctx, copySQL); err != nil {
		return fmt.Errorf("failed to copy endpoints data: %w", err)
	}

	// é‡å»ºç´¢å¼•
	indexSQL := []string{
		"CREATE INDEX IF NOT EXISTS idx_endpoints_channel ON endpoints(channel)",
		"CREATE INDEX IF NOT EXISTS idx_endpoints_priority ON endpoints(priority)",
		"CREATE INDEX IF NOT EXISTS idx_endpoints_enabled ON endpoints(enabled)",
		"CREATE INDEX IF NOT EXISTS idx_endpoints_failover ON endpoints(failover_enabled)",
		"CREATE UNIQUE INDEX IF NOT EXISTS idx_endpoints_channel_name_unique ON endpoints(channel, name)",
	}
	for _, stmt := range indexSQL {
		if _, err := tx.ExecContext(ctx, stmt); err != nil {
			return fmt.Errorf("failed to create endpoints index: %w", err)
		}
	}

	// é‡å»ºè§¦å‘å™¨
	triggerSQL := `
CREATE TRIGGER IF NOT EXISTS update_endpoints_timestamp
    AFTER UPDATE ON endpoints
    FOR EACH ROW
    WHEN NEW.updated_at = OLD.updated_at
BEGIN
    UPDATE endpoints SET updated_at = strftime('%Y-%m-%d %H:%M:%f', 'now', 'localtime') || '+08:00' WHERE id = NEW.id;
END;`
	if _, err := tx.ExecContext(ctx, triggerSQL); err != nil {
		return fmt.Errorf("failed to recreate endpoints trigger: %w", err)
	}

	// åˆ é™¤æ—§è¡¨
	if _, err := tx.ExecContext(ctx, "DROP TABLE endpoints_old"); err != nil {
		return fmt.Errorf("failed to drop endpoints_old: %w", err)
	}

	if err := tx.Commit(); err != nil {
		return fmt.Errorf("failed to commit endpoints rebuild: %w", err)
	}
	s.logger.Info("âœ… [æ•°æ®åº“è¿ç§»] endpointsï¼šå·²é‡å»ºä¸º UNIQUE(channel,name)")
	return nil
}

func (s *SQLiteAdapter) tableExists(ctx context.Context, table string) (bool, error) {
	var count int
	if err := s.db.QueryRowContext(ctx, "SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name = ?", table).Scan(&count); err != nil {
		return false, err
	}
	return count > 0, nil
}

// columnExists æ£€æŸ¥è¡¨ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåˆ—
func (s *SQLiteAdapter) columnExists(ctx context.Context, table, column string) (bool, error) {
	query := fmt.Sprintf("PRAGMA table_info(%s)", table)
	rows, err := s.db.QueryContext(ctx, query)
	if err != nil {
		return false, err
	}
	defer rows.Close()

	for rows.Next() {
		var cid int
		var name string
		var dataType string
		var notNull int
		var dfltValue interface{}
		var pk int

		if err := rows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk); err != nil {
			return false, err
		}

		if name == column {
			return true, nil
		}
	}

	return false, nil
}

// BuildInsertOrReplaceQuery æ„å»ºæ’å…¥æˆ–æ›´æ–°æŸ¥è¯¢ï¼ˆSQLiteè¯­æ³•ï¼‰
// ä½¿ç”¨ INSERT ... ON CONFLICT DO UPDATE æ¥é¿å…æ•°æ®ä¸¢å¤±
func (s *SQLiteAdapter) BuildInsertOrReplaceQuery(table string, columns []string, values []string) string {
	columnsStr := strings.Join(columns, ", ")
	valuesStr := strings.Join(values, ", ")

	// æ„å»ºINSERTéƒ¨åˆ†
	query := fmt.Sprintf("INSERT INTO %s (%s) VALUES (%s)", table, columnsStr, valuesStr)

	// æ„å»ºON CONFLICT DO UPDATEéƒ¨åˆ†ï¼Œå¯¹start_timeå­—æ®µè¿›è¡Œç‰¹æ®Šå¤„ç†
	// å¯¹äºrequest_logsè¡¨ï¼Œä¸»é”®å†²çªæ—¶æ›´æ–°æä¾›çš„å­—æ®µï¼ˆé™¤äº†request_idä¸»é”®ï¼‰
	var updatePairs []string
	for _, col := range columns {
		if col != "request_id" { // è·³è¿‡ä¸»é”®å­—æ®µ
			if col == "start_time" {
				// å¯¹start_timeä½¿ç”¨COALESCEï¼Œåªåœ¨åŸå€¼ä¸ºNULLæ—¶æ‰æ›´æ–°
				updatePairs = append(updatePairs, fmt.Sprintf("%s = COALESCE(request_logs.%s, EXCLUDED.%s)", col, col, col))
			} else {
				updatePairs = append(updatePairs, fmt.Sprintf("%s = EXCLUDED.%s", col, col))
			}
		}
	}

	if len(updatePairs) > 0 {
		query += " ON CONFLICT(request_id) DO UPDATE SET " + strings.Join(updatePairs, ", ")
	} else {
		// å¦‚æœåªæœ‰request_idå­—æ®µï¼Œåˆ™ä½¿ç”¨IGNOREé¿å…é‡å¤æ’å…¥
		query = fmt.Sprintf("INSERT OR IGNORE INTO %s (%s) VALUES (%s)", table, columnsStr, valuesStr)
	}

	return query
}

// BuildDateTimeNow è¿”å›å½“å‰æ—¶é—´å‡½æ•°ï¼ˆæ”¯æŒå¾®ç§’ç²¾åº¦ï¼‰
// SQLiteæ²¡æœ‰æ—¶åŒºæ”¯æŒï¼Œæˆ‘ä»¬åœ¨Goå±‚é¢ç”Ÿæˆæ­£ç¡®æ—¶åŒºçš„æ—¶é—´å­—ç¬¦ä¸²
func (s *SQLiteAdapter) BuildDateTimeNow() string {
	// è·å–å½“å‰é…ç½®æ—¶åŒºçš„æ—¶é—´
	now := time.Now().In(s.location)

	// æ ¼å¼åŒ–ä¸ºSQLiteå…¼å®¹çš„datetimeæ ¼å¼ï¼ˆå¾®ç§’ç²¾åº¦ï¼‰
	return fmt.Sprintf("'%s'", now.Format("2006-01-02 15:04:05.000000"))
}

// BuildLimitOffset æ„å»ºåˆ†é¡µæŸ¥è¯¢
func (s *SQLiteAdapter) BuildLimitOffset(limit, offset int) string {
	if limit <= 0 {
		return ""
	}
	if offset <= 0 {
		return fmt.Sprintf(" LIMIT %d", limit)
	}
	return fmt.Sprintf(" LIMIT %d OFFSET %d", limit, offset)
}

// VacuumDatabase SQLiteæ‰§è¡ŒVACUUMæ“ä½œ
func (s *SQLiteAdapter) VacuumDatabase(ctx context.Context) error {
	s.logger.Info("æ­£åœ¨æ‰§è¡ŒSQLite VACUUMæ“ä½œ")

	_, err := s.db.ExecContext(ctx, "VACUUM")
	if err != nil {
		return fmt.Errorf("failed to vacuum SQLite database: %w", err)
	}

	s.logger.Info("âœ… SQLite VACUUMæ“ä½œå®Œæˆ")
	return nil
}

// GetDatabaseStats è·å–SQLiteæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
func (s *SQLiteAdapter) GetDatabaseStats(ctx context.Context) (*DatabaseStats, error) {
	stats := &DatabaseStats{}

	// è·å–è¯·æ±‚è®°å½•æ€»æ•°
	err := s.db.QueryRowContext(ctx, "SELECT COUNT(*) FROM request_logs").Scan(&stats.TotalRequests)
	if err != nil {
		return nil, fmt.Errorf("failed to get total requests count: %w", err)
	}

	// è·å–æ±‡æ€»è®°å½•æ€»æ•°
	err = s.db.QueryRowContext(ctx, "SELECT COUNT(*) FROM usage_summary").Scan(&stats.TotalSummaries)
	if err != nil {
		return nil, fmt.Errorf("failed to get total summaries count: %w", err)
	}

	// è·å–æœ€æ—©å’Œæœ€æ–°çš„è®°å½•æ—¶é—´
	var earliestStr, latestStr sql.NullString
	err = s.db.QueryRowContext(ctx, "SELECT MIN(start_time), MAX(start_time) FROM request_logs").Scan(&earliestStr, &latestStr)
	if err != nil && err != sql.ErrNoRows {
		return nil, fmt.Errorf("failed to get record time range: %w", err)
	}

	if earliestStr.Valid {
		if t, err := time.Parse(time.RFC3339, earliestStr.String); err == nil {
			stats.EarliestRecord = &t
		}
	}
	if latestStr.Valid {
		if t, err := time.Parse(time.RFC3339, latestStr.String); err == nil {
			stats.LatestRecord = &t
		}
	}

	// è·å–æ•°æ®åº“æ–‡ä»¶å¤§å°ï¼ˆSQLiteç‰¹æœ‰ï¼‰
	var pageCount, pageSize int64
	err = s.db.QueryRowContext(ctx, "PRAGMA page_count").Scan(&pageCount)
	if err == nil {
		err = s.db.QueryRowContext(ctx, "PRAGMA page_size").Scan(&pageSize)
		if err == nil {
			stats.DatabaseSize = pageCount * pageSize
		}
	}

	// è·å–æ€»æˆæœ¬
	err = s.db.QueryRowContext(ctx, "SELECT COALESCE(SUM(total_cost_usd), 0) FROM request_logs WHERE total_cost_usd > 0").Scan(&stats.TotalCostUSD)
	if err != nil && err != sql.ErrNoRows {
		return nil, fmt.Errorf("failed to get total cost: %w", err)
	}

	return stats, nil
}

// GetConnectionStats è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯
func (s *SQLiteAdapter) GetConnectionStats() ConnectionStats {
	if s.db == nil {
		return ConnectionStats{}
	}

	dbStats := s.db.Stats()
	return ConnectionStats{
		OpenConnections:  dbStats.OpenConnections,
		IdleConnections:  dbStats.Idle,
		InUseConnections: dbStats.InUse,
		WaitCount:        dbStats.WaitCount,
		WaitDuration:     dbStats.WaitDuration,
		MaxLifetime:      0, // SQLiteä¸é™åˆ¶è¿æ¥ç”Ÿå‘½å‘¨æœŸ
	}
}

// GetDatabaseType è¿”å›æ•°æ®åº“ç±»å‹æ ‡è¯†
func (s *SQLiteAdapter) GetDatabaseType() string {
	return "sqlite"
}

// diagnoseTimezoneSettings è¯Šæ–­SQLiteæ—¶åŒºè®¾ç½®ï¼Œå¸®åŠ©è°ƒè¯•æ—¶åŒºä¸ä¸€è‡´é—®é¢˜
func (s *SQLiteAdapter) diagnoseTimezoneSettings() {
	// SQLiteæ—¶åŒºè¯Šæ–­ç›¸å¯¹ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨åº”ç”¨å±‚å¤„ç†æ—¶åŒº
	goNow := time.Now()
	goInConfigTZ := time.Now().In(s.location)

	_, goOffset := goInConfigTZ.Zone()
	goOffsetHours := float64(goOffset) / 3600

	s.logger.Info("ğŸ” SQLiteæ—¶åŒºè¯Šæ–­ä¿¡æ¯",
		"configured_timezone", s.location.String(),
		"system_now", goNow.Format("2006-01-02 15:04:05 -07:00"),
		"configured_tz_now", goInConfigTZ.Format("2006-01-02 15:04:05 -07:00"),
		"configured_offset_hours", goOffsetHours,
		"builddatetimenow_output", s.BuildDateTimeNow())

	// éªŒè¯æ—¶åŒºåç§»æ˜¯å¦ç¬¦åˆé¢„æœŸ
	if s.location.String() == "Asia/Shanghai" && goOffsetHours == 8.0 {
		s.logger.Info("âœ… SQLiteæ—¶åŒºè®¾ç½®æ­£ç¡®: ä½¿ç”¨Asia/Shanghaiæ—¶åŒº (+8å°æ—¶)")
	} else if s.location == time.UTC {
		s.logger.Info("â„¹ï¸  SQLiteä½¿ç”¨UTCæ—¶åŒº")
	} else {
		s.logger.Info("â„¹ï¸  SQLiteä½¿ç”¨è‡ªå®šä¹‰æ—¶åŒº", "timezone", s.location.String(), "offset_hours", goOffsetHours)
	}
}

// getSQLiteAppDataDir è·å–åº”ç”¨æ•°æ®ç›®å½•ï¼ˆè·¨å¹³å°ï¼‰
// å¤åˆ¶è‡ª internal/utils/appdir.goï¼Œé¿å…å¾ªç¯ä¾èµ–
// Windows: %APPDATA%\CC-Forwarder
// macOS: ~/Library/Application Support/CC-Forwarder
// Linux: ~/.local/share/cc-forwarder
func getSQLiteAppDataDir() string {
	var baseDir string

	switch runtime.GOOS {
	case "windows":
		baseDir = os.Getenv("APPDATA")
		if baseDir == "" {
			baseDir = filepath.Join(os.Getenv("USERPROFILE"), "AppData", "Roaming")
		}
		return filepath.Join(baseDir, "CC-Forwarder")

	case "darwin":
		homeDir, _ := os.UserHomeDir()
		return filepath.Join(homeDir, "Library", "Application Support", "CC-Forwarder")

	case "linux":
		homeDir, _ := os.UserHomeDir()
		xdgDataHome := os.Getenv("XDG_DATA_HOME")
		if xdgDataHome != "" {
			return filepath.Join(xdgDataHome, "cc-forwarder")
		}
		return filepath.Join(homeDir, ".local", "share", "cc-forwarder")

	default:
		homeDir, _ := os.UserHomeDir()
		return filepath.Join(homeDir, ".cc-forwarder")
	}
}
