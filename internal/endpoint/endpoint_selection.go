// endpoint_selection.go - ç«¯ç‚¹é€‰æ‹©/è·¯ç”±åŠŸèƒ½
// åŒ…å«å¥åº·ç«¯ç‚¹è·å–ã€æ•…éšœè½¬ç§»ç«¯ç‚¹é€‰æ‹©ã€æ’åºç­–ç•¥ç­‰

package endpoint

import (
	"context"
	"fmt"
	"log/slog"
	"sort"
	"strings"
	"time"
)

// GetHealthyEndpoints returns a list of healthy endpoints from active groups based on strategy.
// v6.0: ä»¥â€œæ¸ é“(channel)â€ä¸ºå•ä½è·¯ç”±ï¼Œä¼˜å…ˆåªè¿”å›å½“å‰æ¿€æ´»æ¸ é“å†…çš„ç«¯ç‚¹ï¼›
// è·¨æ¸ é“åˆ‡æ¢ç”±è¯·æ±‚çº§æ•…éšœè½¬ç§»è§¦å‘ï¼ˆè§ TriggerRequestFailoverï¼‰ã€‚
func (m *Manager) GetHealthyEndpoints() []*Endpoint {
	// v5.0+: ä½¿ç”¨å¿«ç…§æœºåˆ¶
	m.endpointsMu.RLock()
	snapshot := make([]*Endpoint, len(m.endpoints))
	copy(snapshot, m.endpoints)
	m.endpointsMu.RUnlock()

	// 1. é¦–å…ˆå°è¯•è·å–æ´»è·ƒç»„ï¼ˆå½“å‰æ¿€æ´»æ¸ é“ï¼‰çš„ç«¯ç‚¹
	activeEndpoints := m.groupManager.FilterEndpointsByActiveGroups(snapshot)

	now := time.Now()
	var healthy []*Endpoint
	for _, endpoint := range activeEndpoints {
		// æ£€æŸ¥æ˜¯å¦å‚ä¸æ•…éšœè½¬ç§»ï¼ˆé»˜è®¤ä¸º trueï¼‰ï¼Œä¸å‚ä¸åˆ™ä¸ä½œä¸ºä»£ç†å€™é€‰
		failoverEnabled := true
		if endpoint.Config.FailoverEnabled != nil {
			failoverEnabled = *endpoint.Config.FailoverEnabled
		}
		if !failoverEnabled {
			continue
		}

		endpoint.mutex.RLock()
		isHealthy := endpoint.Status.Healthy
		// æ£€æŸ¥æ˜¯å¦åœ¨è¯·æ±‚å†·å´ä¸­
		inCooldown := !endpoint.Status.CooldownUntil.IsZero() && now.Before(endpoint.Status.CooldownUntil)
		endpoint.mutex.RUnlock()

		if isHealthy && !inCooldown {
			healthy = append(healthy, endpoint)
		} else if inCooldown {
			slog.Debug(fmt.Sprintf("â­ï¸ [ç«¯ç‚¹é€‰æ‹©] è·³è¿‡å†·å´ä¸­çš„ç«¯ç‚¹: %s", endpoint.Config.Name))
		}
	}

	// 2. å¦‚æœå½“å‰æ¿€æ´»æ¸ é“æœ‰å¯ç”¨ç«¯ç‚¹ï¼Œç›´æ¥è¿”å›
	if len(healthy) > 0 {
		return m.sortHealthyEndpoints(healthy, true)
	}

	// å½“å‰æ¸ é“æ²¡æœ‰å¯ç”¨ç«¯ç‚¹ï¼šç”±ä¸Šå±‚è§¦å‘è·¨æ¸ é“æ•…éšœè½¬ç§»
	return nil
}

// sortHealthyEndpoints sorts healthy endpoints based on strategy with optional logging
func (m *Manager) sortHealthyEndpoints(healthy []*Endpoint, showLogs bool) []*Endpoint {
	// Sort based on strategy
	switch m.config.Strategy.Type {
	case "priority":
		sort.Slice(healthy, func(i, j int) bool {
			return healthy[i].Config.Priority < healthy[j].Config.Priority
		})
	case "fastest":
		// Log endpoint latencies for fastest strategy (only if showLogs is true)
		if len(healthy) > 1 && showLogs {
			slog.Info("ğŸ“Š [Fastest Strategy] åŸºäºå¥åº·æ£€æŸ¥çš„ç«¯ç‚¹å»¶è¿Ÿæ’åº:")
			for _, ep := range healthy {
				ep.mutex.RLock()
				responseTime := ep.Status.ResponseTime
				ep.mutex.RUnlock()
				slog.Info(fmt.Sprintf("  â±ï¸ %s - å»¶è¿Ÿ: %dms (æ¥æº: å®šæœŸå¥åº·æ£€æŸ¥)",
					ep.Config.Name, responseTime.Milliseconds()))
			}
		}

		sort.Slice(healthy, func(i, j int) bool {
			healthy[i].mutex.RLock()
			healthy[j].mutex.RLock()
			defer healthy[i].mutex.RUnlock()
			defer healthy[j].mutex.RUnlock()
			return healthy[i].Status.ResponseTime < healthy[j].Status.ResponseTime
		})
	}

	return healthy
}

// GetFastestEndpointsWithRealTimeTest returns endpoints from active groups sorted by real-time testing.
// v6.0: ä»¥â€œæ¸ é“(channel)â€ä¸ºå•ä½è·¯ç”±ï¼Œåªæµ‹è¯•/æ’åºå½“å‰æ¿€æ´»æ¸ é“å†…ç«¯ç‚¹ã€‚
func (m *Manager) GetFastestEndpointsWithRealTimeTest(ctx context.Context) []*Endpoint {
	// v5.0+: ä½¿ç”¨å¿«ç…§æœºåˆ¶
	m.endpointsMu.RLock()
	snapshot := make([]*Endpoint, len(m.endpoints))
	copy(snapshot, m.endpoints)
	m.endpointsMu.RUnlock()

	// 1. é¦–å…ˆå°è¯•è·å–æ´»è·ƒç»„ï¼ˆå½“å‰æ¿€æ´»æ¸ é“ï¼‰çš„ç«¯ç‚¹
	activeEndpoints := m.groupManager.FilterEndpointsByActiveGroups(snapshot)

	var healthy []*Endpoint
	for _, endpoint := range activeEndpoints {
		// æ£€æŸ¥æ˜¯å¦å‚ä¸æ•…éšœè½¬ç§»ï¼ˆé»˜è®¤ä¸º trueï¼‰ï¼Œä¸å‚ä¸åˆ™ä¸ä½œä¸ºä»£ç†å€™é€‰
		failoverEnabled := true
		if endpoint.Config.FailoverEnabled != nil {
			failoverEnabled = *endpoint.Config.FailoverEnabled
		}
		if !failoverEnabled {
			continue
		}

		endpoint.mutex.RLock()
		if endpoint.Status.Healthy {
			healthy = append(healthy, endpoint)
		}
		endpoint.mutex.RUnlock()
	}

	if len(healthy) == 0 {
		return healthy
	}

	// If not using fastest strategy or fast test disabled, apply sorting with logging
	if m.config.Strategy.Type != "fastest" || !m.config.Strategy.FastTestEnabled {
		return m.sortHealthyEndpoints(healthy, true) // Show logs
	}

	// Check if we have cached fast test results first
	testResults, usedCache := m.fastTester.TestEndpointsParallel(ctx, healthy)

	// Only show health check sorting if we're NOT using cache
	if !usedCache && m.config.Strategy.Type == "fastest" && len(healthy) > 1 {
		slog.InfoContext(ctx, "ğŸ“Š [Fastest Strategy] åŸºäºå¥åº·æ£€æŸ¥çš„æ´»è·ƒç»„ç«¯ç‚¹å»¶è¿Ÿæ’åº:")
		for _, ep := range healthy {
			ep.mutex.RLock()
			responseTime := ep.Status.ResponseTime
			group := ep.Config.Group
			ep.mutex.RUnlock()
			slog.InfoContext(ctx, fmt.Sprintf("  â±ï¸ %s (ç»„: %s) - å»¶è¿Ÿ: %dms (æ¥æº: å®šæœŸå¥åº·æ£€æŸ¥)",
				ep.Config.Name, group, responseTime.Milliseconds()))
		}
	}

	// Log ALL test results first (including failures) - but only if cache wasn't used
	if len(testResults) > 0 && !usedCache {
		slog.InfoContext(ctx, "ğŸ” [Fastest Response Mode] æ´»è·ƒç»„ç«¯ç‚¹æ€§èƒ½æµ‹è¯•ç»“æœ:")
		successCount := 0
		for _, result := range testResults {
			group := result.Endpoint.Config.Group
			if result.Success {
				successCount++
				slog.InfoContext(ctx, fmt.Sprintf("  âœ… å¥åº· %s (ç»„: %s) - å“åº”æ—¶é—´: %dms",
					result.Endpoint.Config.Name, group,
					result.ResponseTime.Milliseconds()))
			} else {
				errorMsg := ""
				if result.Error != nil {
					errorMsg = fmt.Sprintf(" - é”™è¯¯: %s", result.Error.Error())
				}
				slog.InfoContext(ctx, fmt.Sprintf("  âŒ å¼‚å¸¸ %s (ç»„: %s) - å“åº”æ—¶é—´: %dms%s",
					result.Endpoint.Config.Name, group,
					result.ResponseTime.Milliseconds(),
					errorMsg))
			}
		}

		slog.InfoContext(ctx, fmt.Sprintf("ğŸ“Š [æµ‹è¯•æ‘˜è¦] æ´»è·ƒç»„æµ‹è¯•: %dä¸ªç«¯ç‚¹, å¥åº·: %dä¸ª, å¼‚å¸¸: %dä¸ª",
			len(testResults), successCount, len(testResults)-successCount))
	}

	// Sort by response time (only successful results)
	sortedResults := SortByResponseTime(testResults)

	if len(sortedResults) == 0 {
		slog.WarnContext(ctx, "âš ï¸ [Fastest Response Mode] æ´»è·ƒç»„æ‰€æœ‰ç«¯ç‚¹æµ‹è¯•å¤±è´¥ï¼Œå›é€€åˆ°å¥åº·æ£€æŸ¥æ¨¡å¼")
		return healthy // Fall back to health check results if no fast tests succeeded
	}

	// Convert back to endpoint slice
	endpoints := make([]*Endpoint, 0, len(sortedResults))
	for _, result := range sortedResults {
		endpoints = append(endpoints, result.Endpoint)
	}

	// Log the successful endpoint ranking
	if len(endpoints) > 0 {
		// Show the fastest endpoint selection
		fastestEndpoint := endpoints[0]
		var fastestTime int64
		for _, result := range sortedResults {
			if result.Endpoint == fastestEndpoint {
				fastestTime = result.ResponseTime.Milliseconds()
				break
			}
		}

		cacheIndicator := ""
		if usedCache {
			cacheIndicator = " (ç¼“å­˜)"
		}

		slog.InfoContext(ctx, fmt.Sprintf("ğŸš€ [Fastest Response Mode] é€‰æ‹©æœ€å¿«ç«¯ç‚¹: %s - %dms%s",
			fastestEndpoint.Config.Name, fastestTime, cacheIndicator))

		// Show other available endpoints if there are more than one
		if len(endpoints) > 1 && !usedCache {
			slog.InfoContext(ctx, "ğŸ“‹ [å¤‡ç”¨ç«¯ç‚¹] å…¶ä»–å¯ç”¨ç«¯ç‚¹:")
			for i := 1; i < len(endpoints); i++ {
				ep := endpoints[i]
				var responseTime int64
				var epGroup string
				for _, result := range sortedResults {
					if result.Endpoint == ep {
						responseTime = result.ResponseTime.Milliseconds()
						epGroup = result.Endpoint.Config.Group
						break
					}
				}
				slog.InfoContext(ctx, fmt.Sprintf("  ğŸ”„ å¤‡ç”¨ %s (ç»„: %s) - å“åº”æ—¶é—´: %dms",
					ep.Config.Name, epGroup, responseTime))
			}
		}
	}

	return endpoints
}

// GetEndpointByName returns an endpoint by key, only from active groups.
// å…¼å®¹ï¼šYAML æ¨¡å¼ä¸‹ key == nameã€‚
func (m *Manager) GetEndpointByName(endpointKey string) *Endpoint {
	// v5.0+: ä½¿ç”¨å¿«ç…§æœºåˆ¶
	m.endpointsMu.RLock()
	snapshot := make([]*Endpoint, len(m.endpoints))
	copy(snapshot, m.endpoints)
	m.endpointsMu.RUnlock()

	// First filter by active groups
	activeEndpoints := m.groupManager.FilterEndpointsByActiveGroups(snapshot)

	// Then find by name
	for _, endpoint := range activeEndpoints {
		if endpointKeyFromConfig(endpoint.Config) == endpointKey {
			return endpoint
		}
	}
	if endpointKey != "" && !strings.Contains(endpointKey, endpointKeySeparator) {
		var found *Endpoint
		for _, endpoint := range activeEndpoints {
			if endpoint.Config.Name != endpointKey {
				continue
			}
			if found != nil {
				return nil
			}
			found = endpoint
		}
		return found
	}
	return nil
}

// GetEndpointByNameAny returns an endpoint by key from all endpoints (ignoring group status)
// å…¼å®¹ï¼šYAML æ¨¡å¼ä¸‹ key == nameã€‚
func (m *Manager) GetEndpointByNameAny(endpointKey string) *Endpoint {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	// ä¼˜å…ˆæŒ‰ endpointKeyï¼ˆchannel::nameï¼‰æŸ¥æ‰¾
	for _, endpoint := range m.endpoints {
		if endpointKeyFromConfig(endpoint.Config) == endpointKey {
			return endpoint
		}
	}

	// å…¼å®¹ï¼šæ—§è°ƒç”¨æ–¹ä»…ä¼  nameï¼ˆå½“ä¸”ä»…å½“å…¨å±€å”¯ä¸€æ—¶å…è®¸å›é€€ï¼‰
	if endpointKey != "" && !strings.Contains(endpointKey, endpointKeySeparator) {
		var found *Endpoint
		for _, endpoint := range m.endpoints {
			if endpoint.Config.Name != endpointKey {
				continue
			}
			if found != nil {
				// å¤šæ¸ é“åŒåï¼šå›é€€ä¼šäº§ç”Ÿæ­§ä¹‰ï¼Œç›´æ¥è¿”å› nil
				return nil
			}
			found = endpoint
		}
		return found
	}
	return nil
}

// GetAllEndpoints returns all endpoints (deprecated: use GetEndpoints instead)
func (m *Manager) GetAllEndpoints() []*Endpoint {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	result := make([]*Endpoint, len(m.endpoints))
	copy(result, m.endpoints)
	return result
}

// GetEndpoints returns all endpoints for Web interface
func (m *Manager) GetEndpoints() []*Endpoint {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	result := make([]*Endpoint, len(m.endpoints))
	copy(result, m.endpoints)
	return result
}

// GetEndpointStatus returns the status of an endpoint by name
func (m *Manager) GetEndpointStatus(endpointKey string) EndpointStatus {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	for _, ep := range m.endpoints {
		if endpointKeyFromConfig(ep.Config) == endpointKey {
			ep.mutex.RLock()
			status := ep.Status
			ep.mutex.RUnlock()
			return status
		}
	}
	if endpointKey != "" && !strings.Contains(endpointKey, endpointKeySeparator) {
		var found *Endpoint
		for _, ep := range m.endpoints {
			if ep.Config.Name != endpointKey {
				continue
			}
			if found != nil {
				return EndpointStatus{}
			}
			found = ep
		}
		if found != nil {
			found.mutex.RLock()
			status := found.Status
			found.mutex.RUnlock()
			return status
		}
	}
	return EndpointStatus{}
}

// GetEndpointCount è¿”å›å½“å‰ç«¯ç‚¹æ•°é‡ï¼ˆv5.0+ æ–°å¢ï¼‰
func (m *Manager) GetEndpointCount() int {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()
	return len(m.endpoints)
}
